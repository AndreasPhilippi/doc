

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>The Data Driven Newsvendor &mdash; ddop  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Tutorial" href="../tutorial.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> ddop
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../tutorial.html">Tutorial</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">The Data Driven Newsvendor</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Example">Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Getting-Started">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Data">Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#The-classical-Newsvendor-Problem">The classical Newsvendor Problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Parametric-Approach">Parametric Approach</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#Data-Driven-Approach">Data Driven Approach</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Sample-Average-Approximation">Sample Average Approximation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Weighted-Sample-Average-Approximation">Weighted Sample Average Approximation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Empirical-Risk-Minimization">Empirical Risk Minimization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id1">Summary</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ddop</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../tutorial.html">Tutorial</a> &raquo;</li>
        
      <li>The Data Driven Newsvendor</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial_modules/tutorial.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="The-Data-Driven-Newsvendor">
<h1>The Data Driven Newsvendor<a class="headerlink" href="#The-Data-Driven-Newsvendor" title="Permalink to this headline">¶</a></h1>
<p>The newsvendor problem is a classical single period inventory problem, which involves making decisions on order quantities under uncertainty in demand. The traditional way to solve the problem assumes knowledge of the probability distribution of demand. However, in practice, the decision maker does not know the true demand distribution.</p>
<p>In this tutorial you will learn how to solve the newsvendor problem when the underlying demand distribution is unknown but the decision maker has access to historical demand observations. For this purpose the tutorial is structured as follows:</p>
<ul class="simple">
<li><p>In the first part, we will recap the classical newsvendor problem</p></li>
<li><p>In the second part, we will see how to solve the problem by applying the traditional parametric approach</p></li>
<li><p>Finally, in the third part we will cover different data-driven approaches to solve the newsvendor problem.</p></li>
</ul>
<p>Moreover, you will learn how to use our <code class="docutils literal notranslate"><span class="pre">ddop</span></code> python API to apply the different approaches on a real world dataset.</p>
<div class="section" id="Example">
<h2>Example<a class="headerlink" href="#Example" title="Permalink to this headline">¶</a></h2>
<p>In the following, we will consider the real world problem of YAZ. YAZ is a fast casual restaurant in Stuttgart providing great oriental cuisine. The main ingredients for the meals at YAZ, e.g. steak, lamb, fish etc. are prepared at a central factory and are deep-frozen to achieve longer shelf lives. Depending on the estimated demand for the next day, the restaurant manager has to decide how much of the ingredients to defrost over night. These defrosted ingredients/meals then have to be sold
within the following day. If the defrosted quantity was to low to meet the demand this will incur underage cost of <span class="math notranslate nohighlight">\(cu\)</span>. On the other hand, if the quantity was to high, unsold ingredients have to be disposed which in turn will lead to overage cost of <span class="math notranslate nohighlight">\(co\)</span>. Therefore, the store manager wants to choose the order quantity that minimizes the sum of the expected costs. So as you can see, YAZ is facing a typical single period inventory problem (newsvendor problem)</p>
<p><img alt="image1" src="https://drive.google.com/uc?export=view&amp;id=1UPRyUC56wMVd554iHsvOks-MBJaXYwMf" /></p>
</div>
<div class="section" id="Getting-Started">
<h2>Getting Started<a class="headerlink" href="#Getting-Started" title="Permalink to this headline">¶</a></h2>
<p>Before we are ready to start we have to load the libraries we will need in the following.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from ddop.datasets import load_yaz
from ddop.metrics import average_costs
from ddop.newsvendor import SampleAverageApproximationNewsvendor
from ddop.newsvendor import RandomForestWeightedNewsvendor
from ddop.newsvendor import KNeighborsWeightedNewsvendor
from ddop.newsvendor import LinearRegressionNewsvendor
from ddop.newsvendor import DeepLearningNewsvendor
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import scipy
from scipy.stats import norm
</pre></div>
</div>
</div>
<p>You can install missing packages by uncommenting and executing the following cell:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>pip install tensorflow==2.1.0 scikit-learn==0.23.0 keras==2.3.1 ddop seaborn==0.10.1 matplotlib
</pre></div>
</div>
</div>
</div>
<div class="section" id="Data">
<h2>Data<a class="headerlink" href="#Data" title="Permalink to this headline">¶</a></h2>
<p>As mentioned in the beginning, we we will consider the real world problem of Yaz within this tutorial. Therefore, we will use the Yaz dataset that is already included in <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. The dataset stores the demand data for seven main ingredients (calamari, fish, shrimp, chicken, koefte, lamb and steak) over 760 days. In addition, it comes with a bunch of different demand features that we expect to be korrelated to the demand. These features include information about the day, month, year, weather
conditions and more. You can load the dataset using the <code class="docutils literal notranslate"><span class="pre">load_yaz</span></code> function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>yaz = load_yaz(one_hot_encoding=True)
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">load_yaz</span></code> will return an object containing the feature data, the target variables as well as the whole dataframe. You can access the dataframe using the <code class="docutils literal notranslate"><span class="pre">frame</span></code> attribute:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>df = yaz.frame
df
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ISHOLIDAY</th>
      <th>WEEKEND</th>
      <th>WIND</th>
      <th>CLOUDS</th>
      <th>RAINFALL</th>
      <th>HOURS_OF_SUNSHINE</th>
      <th>TEMPERATURE</th>
      <th>WEEKDAY_FRI</th>
      <th>WEEKDAY_MON</th>
      <th>WEEKDAY_SAT</th>
      <th>WEEKDAY_SUN</th>
      <th>WEEKDAY_THU</th>
      <th>WEEKDAY_TUE</th>
      <th>WEEKDAY_WED</th>
      <th>MONTH_APR</th>
      <th>MONTH_AUG</th>
      <th>MONTH_DEC</th>
      <th>MONTH_FEB</th>
      <th>MONTH_JAN</th>
      <th>MONTH_JUL</th>
      <th>MONTH_JUN</th>
      <th>MONTH_MAR</th>
      <th>MONTH_MAY</th>
      <th>MONTH_NOV</th>
      <th>MONTH_OCT</th>
      <th>MONTH_SEP</th>
      <th>YEAR_2013</th>
      <th>YEAR_2014</th>
      <th>YEAR_2015</th>
      <th>CALAMARI</th>
      <th>FISH</th>
      <th>SHRIMP</th>
      <th>CHICKEN</th>
      <th>KOEFTE</th>
      <th>LAMB</th>
      <th>STEAK</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1.9</td>
      <td>7.7</td>
      <td>0.1</td>
      <td>150</td>
      <td>15.9</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>6</td>
      <td>12</td>
      <td>40</td>
      <td>23</td>
      <td>50</td>
      <td>36</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>2.7</td>
      <td>6.9</td>
      <td>10.7</td>
      <td>0</td>
      <td>13.2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>8</td>
      <td>8</td>
      <td>5</td>
      <td>44</td>
      <td>36</td>
      <td>37</td>
      <td>30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>1.4</td>
      <td>8.0</td>
      <td>0.4</td>
      <td>0</td>
      <td>10.6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>5</td>
      <td>11</td>
      <td>19</td>
      <td>12</td>
      <td>22</td>
      <td>16</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>2.3</td>
      <td>6.4</td>
      <td>0.0</td>
      <td>176</td>
      <td>13.3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>4</td>
      <td>2</td>
      <td>28</td>
      <td>13</td>
      <td>28</td>
      <td>22</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>1.7</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>13.5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>7</td>
      <td>4</td>
      <td>9</td>
      <td>22</td>
      <td>18</td>
      <td>29</td>
      <td>29</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>755</th>
      <td>0</td>
      <td>0</td>
      <td>1.6</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>3.5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>8</td>
      <td>47</td>
      <td>30</td>
      <td>32</td>
      <td>32</td>
    </tr>
    <tr>
      <th>756</th>
      <td>0</td>
      <td>0</td>
      <td>1.8</td>
      <td>2.2</td>
      <td>0.0</td>
      <td>362</td>
      <td>14.6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>13</td>
      <td>38</td>
      <td>31</td>
      <td>14</td>
      <td>38</td>
    </tr>
    <tr>
      <th>757</th>
      <td>0</td>
      <td>0</td>
      <td>1.8</td>
      <td>0.7</td>
      <td>0.0</td>
      <td>405</td>
      <td>14.7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>8</td>
      <td>11</td>
      <td>47</td>
      <td>32</td>
      <td>42</td>
      <td>24</td>
    </tr>
    <tr>
      <th>758</th>
      <td>0</td>
      <td>0</td>
      <td>1.9</td>
      <td>6.9</td>
      <td>0.0</td>
      <td>44</td>
      <td>16.0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>6</td>
      <td>9</td>
      <td>50</td>
      <td>40</td>
      <td>44</td>
      <td>32</td>
    </tr>
    <tr>
      <th>759</th>
      <td>0</td>
      <td>1</td>
      <td>1.9</td>
      <td>5.6</td>
      <td>0.0</td>
      <td>46</td>
      <td>17.3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>45</td>
      <td>25</td>
      <td>6</td>
      <td>20</td>
    </tr>
  </tbody>
</table>
<p>760 rows × 36 columns</p>
</div></div>
</div>
<p>Similarly, you can access the feature data using the <code class="docutils literal notranslate"><span class="pre">data</span></code> attribute and the targets with the <code class="docutils literal notranslate"><span class="pre">target</span></code> attribute.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># the feature data
X = yaz.data

# the target data
y = yaz.target
</pre></div>
</div>
</div>
<p>As we want to built and compare different models in the course of this tutorial, we have to split the data into train- and test set. While we use the training set to build a model, we need the test set to evaluate it on unknown data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=False)
</pre></div>
</div>
</div>
<p>Since some models that we are going to consider are sensitive to the variance in the data, we have to normalize the feature values. We therefore use the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> from sklearn. We fit the scaler on the train set and transform both, train- and test data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
</pre></div>
</div>
</div>
</div>
<div class="section" id="The-classical-Newsvendor-Problem">
<h2>The classical Newsvendor Problem<a class="headerlink" href="#The-classical-Newsvendor-Problem" title="Permalink to this headline">¶</a></h2>
<p>Now that we are ready to start, let us first recap the newsvendor problem: The newsvendor problem is a single period inventory problem that refers to a situation in which a seller has to determine the order quantity of perishable goods for the next selling period, after which left-over stock becomes worthless. Hence, at the end of the selling period each unit of unsold stock incurs overage costs of <span class="math notranslate nohighlight">\(co\)</span> and each unit of demand that cannot be satisfied incurs underage costs of <span class="math notranslate nohighlight">\(cu\)</span>.
Therefore, the newsvendor wants to choose the order quantity that minimizes the sum of the expected costs mentioned above.</p>
<p>Given this background, the optimal order quantity for the newsvendor problem can be formulated as follows:</p>
<p><span class="math">\begin{equation}
\min_{q\geq 0} = E_D[cu(D-q)^+ + co(q-D)^+],
\tag{1}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the random future demand, <span class="math notranslate nohighlight">\(q\)</span> is the order quantity, <span class="math notranslate nohighlight">\(cu\)</span> and <span class="math notranslate nohighlight">\(co\)</span> are the per-unit under- and overage costs and <span class="math notranslate nohighlight">\((\cdot)^+ := \max\{0,\cdot\}\)</span> is a function that returns 0 if its argument is negative, and else its argument. If the demand distribution is known, then the optimal decision can be obtained as:</p>
<p><span class="math">\begin{equation}
q^*=F^{-1}\biggl(\frac{cu}{cu+co}\biggl)=F^{-1}(\alpha),
\tag{2}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(F^{-1}(\cdot)\)</span> is the inverse cumulative density function of the demand distribution, and <span class="math notranslate nohighlight">\(\alpha\)</span> is the service level.</p>
</div>
<div class="section" id="Parametric-Approach">
<h2>Parametric Approach<a class="headerlink" href="#Parametric-Approach" title="Permalink to this headline">¶</a></h2>
<p>Now let us go back to our example of YAZ where we want to determine how many ingredients to defrost overnight. If we knew the probability distribution for demand we could simply solve the initial optimization problem <span class="math notranslate nohighlight">\((2)\)</span>. Unfortunately, we don’t know the probability distribution. However, we have access to historical demand samples which we can use to approximate the demand distribution. One way to do this is to fit a parametric distribution to our samples. So let’s do this for a single
product e.g. steak. We start by exploring the data with a histogram. For plotting we are useing <code class="docutils literal notranslate"><span class="pre">seaborn</span></code> - a Python data visualization libary.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># get the demand samples for steak
steak_demand = y_train[&quot;STEAK&quot;]

# print a histogram
sns.distplot(steak_demand, hist=True, norm_hist=False, kde=False,
             hist_kws={&#39;edgecolor&#39;:&#39;black&#39;})

plt.ylabel(&#39;Frequnecy&#39;)
plt.xlabel(&#39;Demand&#39;)

plt.show(sns)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_modules_tutorial_22_0.png" src="../_images/tutorial_modules_tutorial_22_0.png" />
</div>
</div>
<p>This reminds of a normal distribution, doesn’t it? So let us fit a normal distribution to our data by estimation the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine mean and standard deviation
mean = round(y_train[&quot;STEAK&quot;].mean(),2)
std = round(y_train[&quot;STEAK&quot;].std(),2)

# plot histogram and fit a normal distribution to it
sns.distplot(steak_demand, hist=True, fit=scipy.stats.norm, norm_hist=False,
             kde=False, hist_kws={&#39;edgecolor&#39;:&#39;black&#39;})

plt.ylabel(&#39;Empirical probability&#39;)
plt.xlabel(&#39;Demand&#39;)
plt.title(&quot;Fit result: mean = %.2f,  std = %.2f&quot; % (mean, std))

plt.show(sns)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_modules_tutorial_24_0.png" src="../_images/tutorial_modules_tutorial_24_0.png" />
</div>
</div>
<p>So now we have a normal distribution with mean = 23.37 and standard deviation = 10.24 fitted to the data. Instead of using the true demand distribution (which we do not know), we use this distribution to determine how many steaks to defrost. But before we have to define the under- and overage costs for steak. The steak manager tells us, that each unit of unsold steak costs Yaz 5€, and each unit of demand that cannot be met because Yaz has run out of inventory costs 15€.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>cu = 15
co = 5
</pre></div>
</div>
</div>
<p>Using these information, the newsvendor problem then tells us how many steaks to defrost overnight.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine optimal inventory quantity
q = round(norm(mean, std).ppf(cu/(cu+co)))
q
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
30
</pre></div></div>
</div>
<p>We call this the traditional parametric approach, we first assume the demand falls in a family of parametric distributions, estimate its parameters, and then solve the initial optimization problem.</p>
<p>To see how good this approach works we can calculate the average costs on the training set associated with our decision. Therefore, we use the <code class="docutils literal notranslate"><span class="pre">average_costs</span></code> function implemented in <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. The function takes four arguments in the following order - the true values, the predicted values, the underage costs and the overage costs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># create array with lenght of the test set and inventory quantity q
y_pred = np.full(y_test[&quot;STEAK&quot;].shape[0],q)

# calculate and print average costs
avg_costs = average_costs(y_test[&quot;STEAK&quot;], y_pred, cu, co)
print(&quot;Average costs: &quot;+str(avg_costs))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Average costs: [65.60526316]
</pre></div></div>
</div>
<p>As you can see we have average cost of 65.61€ associated with our decision. Now the question is whether we can do better. To answer this question let us go back to our data. So far we only used historical steak demand samples. However, we have access to exogenous features that we expect to have predictive power for demand. So, what we are doing next is to look if we can find a feature that correlates with the demand. We can for example inspect if the day of the week has an impact. Therefore, we
estimate the mean demand for steak for each weekday.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>days = [&#39;MON&#39;,&#39;TUE&#39;,&#39;WED&#39;,&#39;THU&#39;,&#39;FRI&#39;,&#39;SAT&#39;,&#39;SUN&#39;]
mean_per_day = []
for day in days:
  data = pd.concat([X_train, y_train], axis=1)
  mean = data[data[&quot;WEEKDAY_&quot;+day] == 1][&quot;STEAK&quot;].mean()
  mean_per_day.append(mean)
  print(day,&quot;: &quot;,round(mean,2))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
MON :  18.79
TUE :  20.27
WED :  21.8
THU :  21.89
FRI :  26.43
SAT :  37.8
SUN :  16.55
</pre></div></div>
</div>
<p>As we can see the demand varies a lot depending on the day of the week. While the most steak is sold on Saturdays, the least is sold on Sundays and Mondays. One way to take this information into consideration is to fit a normal distribution for each weekday.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine standard deviation for each weekday
std_per_day = []
for day in days:
  data = pd.concat([X_train, y_train], axis=1)
  std = data[data[&quot;WEEKDAY_&quot;+day] == 1][&quot;STEAK&quot;].std()
  std_per_day.append(std)
  print(day,&quot;: &quot;,round(std,2))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
MON :  7.44
TUE :  6.33
WED :  6.54
THU :  6.79
FRI :  8.06
SAT :  12.21
SUN :  6.08
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># plot
fig, ((ax1, ax2, ax3), (ax4, ax5, ax6),
      (ax7, ax8, ax9)) = plt.subplots(3, 3, figsize=(12,12))

for i in range(9):
  if i&lt;=6:
    steak_demand_day = df[df[&#39;WEEKDAY_&#39;+days[i]]==1][&#39;STEAK&#39;]

    sns.distplot(steak_demand_day, ax=fig.axes[i],
                 fit=scipy.stats.norm,kde=False, hist_kws={&#39;edgecolor&#39;:&#39;black&#39;})
    fig.axes[i].set_title(&quot;%s: mean = %.2f,  std = %.2f&quot; % (days[i], mean_per_day[i], std_per_day[i]))

  else:
    fig.axes[i].set_frame_on(False)
    fig.axes[i].get_yaxis().set_visible(False)

  fig.axes[i].set_ylabel(&#39;Empirical probability&#39;)
  fig.axes[i].set_xlabel(&#39;Demand&#39;)
  fig.axes[i].label_outer()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_modules_tutorial_35_0.png" src="../_images/tutorial_modules_tutorial_35_0.png" />
</div>
</div>
<p>Given these distributions, we can write a simple predict function that solves the newsvendor problem depending on the weekday:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def predict(X):
  &quot;&quot;&quot;
  For each row in X, check which day of the week it is, get the mean and
  standard deviation of the corresponding  distribution and then calculate
  the optimal inventory quantity.

  Parameters
  ----------
  X : Pandas DataFrame of shape (n_samples, n_features)
      The input samples to predict.

  Returns
  ----------
  y : array of shape (n_samples,)
      The predicted values
  &quot;&quot;&quot;

  pred = []
  for index, row in X.iterrows():
    if row[&#39;WEEKDAY_MON&#39;]==1:
      mean = mean_per_day[0]
      std = std_per_day[0]
    elif row[&#39;WEEKDAY_TUE&#39;]==1:
      mean = mean_per_day[1]
      std = std_per_day[1]
    elif row[&#39;WEEKDAY_WED&#39;]==1:
      mean = mean_per_day[2]
      std = std_per_day[2]
    elif row[&#39;WEEKDAY_THU&#39;]==1:
      mean = mean_per_day[3]
      std = std_per_day[3]
    elif row[&#39;WEEKDAY_FRI&#39;]==1:
      mean = mean_per_day[4]
      std = std_per_day[4]
    elif row[&#39;WEEKDAY_SAT&#39;]==1:
      mean = mean_per_day[5]
      std = std_per_day[5]
    else:
      mean = mean_per_day[6]
      std = std_per_day[6]

    q = round(norm(mean, std).ppf(cu/(cu+co)))
    pred.append(q)

  return np.array(pred)
</pre></div>
</div>
</div>
<p>Then we apply the function to the test data and calculate the average costs again.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># predict optimal inventory quantity
y_pred = predict(X_test)

# calculate and print average costs
avg_costs = average_costs(y_test[&quot;STEAK&quot;], y_pred, cu, co)
print(&quot;Average costs: &quot;+str(avg_costs))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Average costs: [54.07894737]
</pre></div></div>
</div>
<p>Now look, we were able to reduce the average cost from 65.61€ to 54.08€. That is a great improvment! But maybe we can do even better by considering the other features of the dataset as well. To do this, we can fit a distribution for samples with the same features - like we did before but now for more features than just the weekday. But this has two main drawbacks: 1. Let us say we just have two features, the weekday and the month. If we would like to fit a distribution for each feature
combination we would have to fit 7*12=84 distributions. This sounds like a lot of work, doesn’t it? 2. Now consider the case where it is monday and january. As you can see below we only have 8 samples with the same feature attributes. Such a small number of samples makes it hard to fit a meaningful distribution. Moreover, with increasing number of features we may not have a single sample with the same values.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train[(X_train[&#39;WEEKDAY_MON&#39;]==1)&amp;(X_train[&#39;MONTH_JAN&#39;]==1)].shape[0]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
8
</pre></div></div>
</div>
<p>Instead of fitting a distribution for samples with the same features, we can fit a machine-learning model to predict the demand. For example we can use a decision tree. We therefore use the <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># train a decision tree regressor
mdl = DecisionTreeRegressor(max_depth=5, random_state=1)
mdl.fit(X_train,y_train[&quot;STEAK&quot;])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
DecisionTreeRegressor(max_depth=5, random_state=1)
</pre></div></div>
</div>
<p>Of course we cannot assume the model to be perfect: first because of the model error itself, and second, because of the uncertainty in demand. For this reason, we need to adjust the predictions for uncertainty to obtain optimal decisions. We can get a representation of the remaining uncertainty by evaluating the distribution of the prediction error on the training data. Assuming the error to be distributed normal, we estimate its parameters <span class="math notranslate nohighlight">\(\mu_{e}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{e}\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># predict on training data
train_predicitons = mdl.predict(X_train)

# estimate mean and standard deviation of the model error
error = y_train[&quot;STEAK&quot;]-train_predicitons
error_mean = error.mean()
error_std = error.std()
</pre></div>
</div>
</div>
<p>We then pass the error distribution to the initial optimization problem to determine an additional safety stock. The final order decision is then the sum of both the prediction generated by our model and the safety stock. More formally, the newsvendor problem can then be described as:</p>
<p><span class="math">\begin{equation}
q(x)^{*} = \mu(x)+\Phi^{-1}(\alpha),
\tag{3}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(\mu(\cdot)\)</span> is the function of the machine-learning model that maps from the feature vector <span class="math notranslate nohighlight">\(x\)</span> to demand level, and <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> the inverse cdf of the error distribution with mean <span class="math notranslate nohighlight">\(\mu_{e}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{e}\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine safety buffer
safety_buffer = norm(error_mean, error_std).ppf(cu/(cu+co))

# predict test data
pred = mdl.predict(X_test)

# add safety buffer prediction from model
pred = pred + safety_buffer

# calculate and print average costs
avg_costs = average_costs(y_test[&quot;STEAK&quot;], pred, cu, co)
print(&quot;Average costs: &quot;+str(avg_costs))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Average costs: [45.5082525]
</pre></div></div>
</div>
<p>Again, we were able to reduce our average cost from 51.97€ to 45.51€.</p>
<div class="section" id="Summary">
<h3>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¶</a></h3>
<p>Let us now summarize what we have learned so far:</p>
<ul class="simple">
<li><p>In case we don’t know the demand distribution, but have access to historical demand observations, we can fit a distribution to the historical samples and then solve the initial problem.</p></li>
<li><p>We can improve our inventory decision by taking into account features. To do this, we can fit a distribution for samples with the same features. However, this does not work well for a big number of features.</p></li>
<li><p>Instead, we can fit a machine-learning model to predict demand and, given its error distribution, calculate an additional safety stock. The optimal decision is then the sum of prediction and safety stock.</p></li>
</ul>
</div>
</div>
<div class="section" id="Data-Driven-Approach">
<h2>Data Driven Approach<a class="headerlink" href="#Data-Driven-Approach" title="Permalink to this headline">¶</a></h2>
<p>In this part of the tutorial, we introduce different <strong>“data driven”</strong> approaches to solve the newsvendor problem. In contrast to the traditional way of first estimating the demand and then solving the initial optimization problem, these approches directly prescribes decisions from data.</p>
<div class="section" id="Sample-Average-Approximation">
<h3>Sample Average Approximation<a class="headerlink" href="#Sample-Average-Approximation" title="Permalink to this headline">¶</a></h3>
<p>In the case, where only historical demand data is available, a sensible data-driven approach is to substitute the true expectation with a sample average expectations and solve the resulting optimization problem:</p>
<p><span class="math">\begin{equation}q^{*}=\min _{q \geq 0} \frac{1}{n} \sum_{i=1}^{n}\left[c u\left(d_{i}-q\right)^{+}+c o\left(q-d_{i}\right)^{+}\right]
\tag{4}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the total number of samples and <span class="math notranslate nohighlight">\(d_i\)</span> is the ith demand observation. This approach is called <strong>sample average approximation (SAA)</strong>. To fully understand how this approach works, lets go through a simple example.</p>
<p><strong>Example:</strong> Determine how many steaks to defrost applying <strong>SAA</strong>. The historic demand for steak is given by: <span class="math notranslate nohighlight">\(D=[27,29,30]\)</span>. You can sell steak in your restaurant for 15€ (underage costs). Unsold units incur overage costs of 5€.</p>
<p>Let’s now try to minimize the optimization problem:</p>
<p><span class="math">\begin{equation}
.....\\
q=27: \frac{1}{3}\Bigl[15*(27-27)+15*(29-27)+15*(30-27)\Bigl]=\frac{1}{3}(15*0+15*2+5*3)=25\\
q=28: \frac{1}{3}\Bigl[5*(28-27)+15*(29-28)+15*(30-28)\Bigl]=\frac{1}{3}(5*1+15*1+15*2)=16,67\\
q=29: \frac{1}{3}\Bigl[5*(29-27)+15*(29-29)+15*(30-29)\Bigl]=\frac{1}{3}(5*2+15*0+15*1)=8,33\\
q=30: \frac{1}{3}\Bigl[5*(30-27)+5*(30-29)+15*(30-30)\Bigl]=\frac{1}{3}(5*3+5*1+15*0)=6,67\\
q=31: \frac{1}{3}\Bigl[5*(31-27)+5*(31-29)+5*(31-30)\Bigl]=\frac{1}{3}(5*4+5*2+5*1)=11,67\\
.....
\end{equation}</span></p>
<p>As you can see, we are trying to find the decision <span class="math notranslate nohighlight">\(q\)</span> that minimizes the average costs on our historical samples. Consequently, in this case the optimal decision is given by <span class="math notranslate nohighlight">\(q=30\)</span>.</p>
<p>Now that we know how SAA works let us apply the approach on our dataset. We can therefore use the <code class="docutils literal notranslate"><span class="pre">SampleAverageApproximationNewsvendor</span></code> class from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. To initialize the model we just have to pass the under- and overage costs. Then we can fit the model to the historical demand samples and predict the optimal decision.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>SAA = SampleAverageApproximationNewsvendor(cu,co)
SAA.fit(y_train[&quot;STEAK&quot;])
SAA.predict()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[28]])
</pre></div></div>
</div>
<p>We see, the optimal decision is 28. To calculate the average cost on the training data, we can use the model’s score function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>SAA.score(y_test[&quot;STEAK&quot;])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-60.23684210526316
</pre></div></div>
</div>
</div>
<div class="section" id="Weighted-Sample-Average-Approximation">
<h3>Weighted Sample Average Approximation<a class="headerlink" href="#Weighted-Sample-Average-Approximation" title="Permalink to this headline">¶</a></h3>
<p>Eventhough SAA is a common and effective approach, it only considers historical demand data. However, in the first part of the tutorial we have seen that demand depends on exogenous factors such as the day. As first step, we therefore fitted a distribution for each weekday. We can now do the same here by fitting a SAA model for each weekday. This means that we are trying to find the decision <span class="math notranslate nohighlight">\(q\)</span> that minimizes the average cost for only samples with the same day instead of trying to find
the decision <span class="math notranslate nohighlight">\(q\)</span> that minimizes the average cost for all historical samples.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>days = [&#39;MON&#39;,&#39;TUE&#39;,&#39;WED&#39;,&#39;THU&#39;,&#39;FRI&#39;,&#39;SAT&#39;,&#39;SUN&#39;]

pred = []

SAA = SampleAverageApproximationNewsvendor(cu,co)

for day in days:
  steak_demand_day = df[df[&#39;WEEKDAY_&#39;+day]==1][&#39;STEAK&#39;]
  SAA.fit(steak_demand_day)
  pred.append(SAA.predict().item(0))

print(pred)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[21, 23, 26, 25, 30, 44, 21]
</pre></div></div>
</div>
<p>Now ginve the optimla decision for each day, we can write a new prediction function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def predict(X):
  &quot;&quot;&quot;
  Get the prediction for each sample in X depending on the weekday.

  Parameters
  ----------
  X : Pandas DataFrame of shape (n_samples, n_features)
      The input samples to predict.

  Returns
  ----------
  y : array of shape (n_samples,)
      The predicted values
  &quot;&quot;&quot;

  pred = []
  for index, row in X.iterrows():
    if row[&quot;WEEKDAY_MON&quot;]==1:
      pred.append(21)
    elif row[&quot;WEEKDAY_TUE&quot;]==1:
      pred.append(23)
    elif row[&quot;WEEKDAY_WED&quot;]==1:
      pred.append(26)
    elif row[&quot;WEEKDAY_THU&quot;]==1:
      pred.append(25)
    elif row[&quot;WEEKDAY_FRI&quot;]==1:
      pred.append(30)
    elif row[&quot;WEEKDAY_SAT&quot;]==1:
      pred.append(44)
    else:
      pred.append(21)

  return np.array(pred)
</pre></div>
</div>
</div>
<p>We can then apply the function on the test set and calculate the average costs:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>pred = predict(X_test)
average_costs(y_test[&quot;STEAK&quot;],pred,cu,co)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([50.5])
</pre></div></div>
</div>
<p>Look, again we were able to reduce the average cost from 60.24€ to 50.50€ by taking into account the weekday. We could now go a step further, by fitting a model for samples with the same features. However, as we have seen in part 1, this is not the best way. Another way to take the feature information into account is to give higher weights to samples whose features are similar to those of a new instance. In other words, we can determine a weight for each historical observation based on the
similarity to a new instance, and optimize the initial problem against a re-weighting of the data. To determine the sample weights we can use machine-learning techniques - for example a regression tree. To see how we can do this let us consider a small example.</p>
<p>Let’s say we have the following historical samples with two features (Weekend,Temperature) and we want to calculate their weights (representing the similarity) based on a new sample <span class="math notranslate nohighlight">\(x=(0,18)\)</span>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 50%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Sample</p></th>
<th class="head"><p>Weekend</p></th>
<th class="head"><p>Temperature [in °C]</p></th>
<th class="head"><p>Demand</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>19</p></td>
<td><p>27</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>25</p></td>
<td><p>29</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>1</p></td>
<td><p>23</p></td>
<td><p>30</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>0</p></td>
<td><p>25</p></td>
<td><p>18</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>0</p></td>
<td><p>24</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>0</p></td>
<td><p>22</p></td>
<td><p>23</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>0</p></td>
<td><p>11</p></td>
<td><p>21</p></td>
</tr>
</tbody>
</table>
<p>First, we fit a regression tree on the historical samples, which gives us the following result:</p>
<p><img alt="43daee84b454419c833b3c862607c0fc" class="no-scaled-link" src="https://drive.google.com/uc?export=view&amp;id=1fXW6PV2bUcnZSNByTde39k8hEgRB_5aV" style="width: 450px;" /></p>
<p>As we can see, the regression tree splits our data into four leafs based on the two features. In the next step we can use this tree to determine the sample weights. Therefore, we look to which leaf the new instance <span class="math notranslate nohighlight">\(x=(0,18)\)</span> belongs. In the first level of the tree we follow the right path since it is no weekend. Then, given the temperature forecast of <span class="math notranslate nohighlight">\(18°\)</span> we end up in leaf 1 together with sample 1,6 and 7, which means that these observations are most similar to <span class="math notranslate nohighlight">\(x\)</span>. Using
this information we assign a weight of <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span> to each of the three samples falling into the same leaf. The sample weights are then given by:</p>
<p><span class="math">\begin{equation}
w_1=1/3,\:w_2=0,\:w_3=0,\:w_4=0,\:w_5=0,\:w_6=1/3,\:w_7=1/3
\end{equation}</span></p>
<p>Now that we have calculated the sample weights we can solve the optimization problem like we did before. The only difference now is, that we multiply each observation with its corresponding weight, which indicates the similarity to the new sample.</p>
<p><span class="math">\begin{equation}
q=18: \frac{1}{3}\bigl[15*(27-18)\bigl]+0\bigl[15*(29-18)\bigl]+...+0\bigl[15*(20-18)\bigl]+\frac{1}{3}\bigl[15*(23-18)\bigl]+\frac{1}{3}\bigl[15*(21-18)\bigl] = 85\\
......\\
q=26: \frac{1}{3}\bigl[15*(27-26)\bigl]+0\bigl[15*(29-26)\bigl]+...+0\bigl[5*(26-20)\bigl]+\frac{1}{3}\bigl[5*(26-23)\bigl]+\frac{1}{3}\bigl[5*(26-21)\bigl] = 18.33\\
q=27: \frac{1}{3}\bigl[15*(27-27)\bigl]+0\bigl[15*(29-27)\bigl]+...+0\bigl[5*(27-20)\bigl]+\frac{1}{3}\bigl[5*(27-23)\bigl]+\frac{1}{3}\bigl[5*(27-21)\bigl] = 16.67\\
q=28: \frac{1}{3}\bigl[5*(28-27)\bigl]+0\bigl[15*(29-28)\bigl]+...+0\bigl[5*(28-20)\bigl]+\frac{1}{3}\bigl[5*(28-23)\bigl]+\frac{1}{3}\bigl[5*(28-21)\bigl] = 21.67\\
......\\
q=30: \frac{1}{3}\bigl[5*(30-27)\bigl]+0\bigl[5*(30-29)\bigl]+...+0\bigl[5*(30-20)\bigl]+\frac{1}{3}\bigl[5*(30-23)\bigl]+\frac{1}{3}\bigl[5*(30-21)\bigl] = 30.67
\end{equation}</span></p>
<p>So, as we can see from our calculation, <span class="math notranslate nohighlight">\(q=27\)</span> is the quantity minimizing the average costs on the weighted samples.</p>
<p>We call this approach <strong>“Weigthed Sample Average Approximation (wSAA)”</strong> as it can be seen as weighted form of SAA. More formally the problem can be stated as follow:</p>
<p><span class="math">\begin{equation}
q(x)^*=\min_{q\geq 0} \sum_{i=1}^{n}w_i(x)\bigl[cu(d_i-q)^+ + co(q-d_i)\bigl],
\tag{5}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(w_i(x) \in [0,1]\)</span> is the weight of the ith sample based on a new instance <span class="math notranslate nohighlight">\(x\)</span>. The weight function <span class="math notranslate nohighlight">\(w_i(\cdot)\)</span> can be considered a similarity function. In our example above we used a weight function based on a tree regressor given by:</p>
<p><span class="math">\begin{equation}
w_{i}^{Tree}(x)=\frac{\mathbb{1}x_i \in R(x,\theta)}{N(x,\theta)},
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(\mathbb{1}\)</span> is the indicator function, <span class="math notranslate nohighlight">\(x\)</span> is the feature vector of a new sample, <span class="math notranslate nohighlight">\(x_i\)</span> is the feature vector of the ith sample, <span class="math notranslate nohighlight">\(R(x)\)</span> is the leaf containing <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(N(x)\)</span> is the number of samples falling in leaf <span class="math notranslate nohighlight">\(R(x)\)</span>. In simple terms, the function will give a wight of <span class="math notranslate nohighlight">\(\frac{1}{N(x,\theta)}\)</span> to the ith sample <span class="math notranslate nohighlight">\(x_i\)</span> if it belongs to the same leaf as the new instance <span class="math notranslate nohighlight">\(x\)</span>. If not, a weight of <span class="math notranslate nohighlight">\(0\)</span> is assigned.</p>
<p>Obviously, the performance of a wSAA approach is driven by the choice of weight function. In the following we therefore introduce two other weight functions based on k-nearest-neighbor- and random forest regression.</p>
<p><strong>Random forest:</strong></p>
<p><span class="math">\begin{equation}
w_{i}^{RF}(x)=\frac{1}{T} \sum_{t=1}^{T} \frac{\mathbb{1}_{x_i \in R(x, \theta_t)}}{N(x,\theta_t)},
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the number of trees in the forest, <span class="math notranslate nohighlight">\(R(x,\theta_t)\)</span> is the leaf of tree <span class="math notranslate nohighlight">\(t\)</span> containing <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(N(x,\theta_t)\)</span> is the number of samples falling in leaf <span class="math notranslate nohighlight">\(R(x,\theta_t)\)</span>.</p>
<p><strong>K-nearest-neighbor:</strong></p>
<p><span class="math">\begin{equation}
w_{i}^{k \mathrm{NN}}(x)=\frac{1}{k} \mathbb{1}\left[x_i \text { is a } k \mathrm{NN} \text { of } x\right],
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the number of neighbors. In simple terms, the function will give a weight of <span class="math notranslate nohighlight">\(\frac{1}{k}\)</span> to sample <span class="math notranslate nohighlight">\(x_i\)</span> if it is a k-nearest-neighbor of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>We now dont want to go into more detail. Instead, let us apply wSAA on our dataset. We can therefore use the class <code class="docutils literal notranslate"><span class="pre">RandomForestWeightedNewsvendor</span></code> and <code class="docutils literal notranslate"><span class="pre">KNeighborsWeightedNewsvendor</span></code> from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. To initialize the models, we only have to pass our overage- and underage costs. However, there are more model specific parameters that we should modify. For instance, we set the maximum depth for <code class="docutils literal notranslate"><span class="pre">RandomForestWeightedNewsvendor</span></code> to 5 to avoid overfitting. Moreover, we set the number of neighbors
for <code class="docutils literal notranslate"><span class="pre">KNeighborsWeightedNewsvendor</span></code> to 30. After we have initialized our models we fit them on the training data and calculate the average cost on the test set by using the <code class="docutils literal notranslate"><span class="pre">score</span></code> function:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>cu = 15
co = 5

RFW = RandomForestWeightedNewsvendor(cu=cu, co=co, max_depth=5, criterion=&quot;mse&quot;,
                                     random_state=3)
RFW.fit(X_train, y_train[&quot;STEAK&quot;])
RFW_score = RFW.score(X_test,y_test[&quot;STEAK&quot;])
print(&quot;Avg. cost RFW: &quot;,RFW_score)

KNW = KNeighborsWeightedNewsvendor(cu=cu, co=co, n_neighbors=30)
KNW.fit(X_train_scaled, y_train[&quot;STEAK&quot;])
KNW_score = KNW.score(X_test_scaled,y_test[&quot;STEAK&quot;])
print(&quot;Avg. cost KNW: &quot;, KNW_score)

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Avg. cost RFW:  -47.76315789473684
Avg. cost KNW:  -47.473684210526315
</pre></div></div>
</div>
<p>As we can see we were able to reduce the average costs again by using <strong>wSAA</strong>.</p>
<hr class="docutils" />
</div>
<div class="section" id="Empirical-Risk-Minimization">
<h3>Empirical Risk Minimization<a class="headerlink" href="#Empirical-Risk-Minimization" title="Permalink to this headline">¶</a></h3>
<p>With wSAA we have already learned a data-driven approach that is able to consider features for decision making. However, to obtain a decision <span class="math notranslate nohighlight">\(q\)</span>, we have to first determine weights and then solve an optimization problem for every new sample <span class="math notranslate nohighlight">\(x\)</span>. But wouldn’t it be great to learn a function that instead maps directly from features <span class="math notranslate nohighlight">\(x\in X\)</span> to a decision <span class="math notranslate nohighlight">\(q\in Q\)</span>. A natural way to obtain such a function, given past demand samples (containing both features and demand), is
though <a class="reference external" href="https://en.wikipedia.org/wiki/Empirical_risk_minimization">empirical risk minimization</a>: <span class="math">\begin{equation}
\min_{q(\cdot)\in\mathcal{F}} \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-q(x_i))^+ + co(q(x_i)-d_i)^+\bigl],
\tag{6}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is the function that maps from feature space <span class="math notranslate nohighlight">\(X\)</span> to decision space <span class="math notranslate nohighlight">\(Q\)</span>, and <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is its function class. Again, <span class="math notranslate nohighlight">\(x_i\)</span> is the feature vector of the i-th sample, and <span class="math notranslate nohighlight">\(d_i\)</span> is the corresponding demand value. To say it simple, we try to find the function <span class="math notranslate nohighlight">\(q:X\rightarrow Q\)</span>, that minimizes the average costs on <span class="math notranslate nohighlight">\(n\)</span> past demand samples (the empirical risk). The most straightforward way to do this is based on linear regression. So
let’s look at a simple example:</p>
<div class="section" id="Linear-Regression">
<h4>Linear Regression<a class="headerlink" href="#Linear-Regression" title="Permalink to this headline">¶</a></h4>
<p>Suppose we consider only one demand feature, say temperature, and given the temperature we want to know how many steaks to defrost. In other words, we want to find a function <span class="math notranslate nohighlight">\(q(temp)\)</span>, that maps from temperature to a decision <span class="math notranslate nohighlight">\(q\)</span>. Of course, the first thing we need to do is to collect some data.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 19%" />
<col style="width: 61%" />
<col style="width: 19%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Sample</p></th>
<th class="head"><p>Temperature [in °C]</p></th>
<th class="head"><p>Demand</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>10</p></td>
<td><p>5</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>20</p></td>
<td><p>10</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>30</p></td>
<td><p>9</p></td>
</tr>
</tbody>
</table>
<p>We then plot the data and as we can see it looks like there is a linear relationship between temperature and demand. Consequently, we want to find a linear function <span class="math notranslate nohighlight">\(q(temp)=b+w_1*temp\)</span> that fits best on our data. As you can see, we therefore have to learn two variables, the intercept term <span class="math notranslate nohighlight">\(b\)</span> and a weight <span class="math notranslate nohighlight">\(w_1\)</span>. In other words, we want to have a constant value to which we add a certain amound depending on the temperature. We start by selecting two random values,
e.g. <span class="math notranslate nohighlight">\(b=0\)</span> and <span class="math notranslate nohighlight">\(w_1=0,4\)</span>, which gives us function <span class="math notranslate nohighlight">\(q_1(temp)=0+0,4*temp\)</span></p>
<p><img alt="e84579e7320c4cb295594d200d9af277" class="no-scaled-link" src="https://drive.google.com/uc?export=view&amp;id=1TC3-K8j_7hsZcJuf1RrKhyvtdwTfCkSs" style="width: 550px;" /></p>
<p>Given function <span class="math notranslate nohighlight">\(q_1(\cdot)\)</span> we can determine the decision <span class="math notranslate nohighlight">\(q\)</span> for each sample in our data: <span class="math">\begin{equation}
q_1(10)=4\\
q_1(20)=8\\
q_1(30)=12\\
\end{equation}</span></p>
<p>In the next step, we can compare the results with the actual demand. Of course, we are interested in making cost-optimal decisions. So, to see how well the function fits our data, we can determine the average cost associated with our decisions. Recall that each unit of unsold steak costs Yaz 5€, and each unit of demand that cannot be met because Yaz has run out of inventory costs 15€, we can continue as follows:</p>
<p><span class="math">\begin{equation}
\frac{1}{3}[15*(5-4)+0+5*(12-10)]=8.33
\end{equation}</span></p>
<p>As we can see, function <span class="math notranslate nohighlight">\(q_1(\cdot)\)</span> yields average costs of 8.33€. This is really not a good result. Let us therefore change the values for <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span>, and define a function <span class="math notranslate nohighlight">\(q_2(temp)=4+0,2*temp\)</span>. Following the same procedure as before gives us:</p>
<p><span class="math">\begin{equation}
q_2(10)=6\\
q_2(20)=8\\
q_2(30)=10\\
\frac{1}{3}[5*(6-5)+0+5*(10-9)]=3.33
\end{equation}</span></p>
<p>Look, using function <span class="math notranslate nohighlight">\(q_2(\cdot)\)</span> we were able to reduce the cost down to 3.33. But maybe we can do even better. We could now do the same with other values for <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(w_1\)</span> over and over again until we find the function that minimizes the average cost on the training samples. But this sounds very tedious doesn’t it? Instead, we can let a computer do this work. For this, however, we need to formalize the optimization problem at hand:</p>
<p><span class="math">\begin{equation}
\min_{b,w_1} \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-(b+w_1*temp))^+ + co((b+w_1*temp)-d_i)^+\bigl]
\end{equation}</span></p>
<p>Doesn’t this look like equation <span class="math notranslate nohighlight">\((6)\)</span>, which we introduced at the very beginning of this chapter? The only difference is that we have defined <span class="math notranslate nohighlight">\(q(\cdot)\)</span> as <span class="math notranslate nohighlight">\(q(temp)=b+w_1*temp\)</span>. To be more precise, we have assumed that temperature and demand have a linear relationship. Luckily we don’t have to implement the optimization problem ourselves to solve it on our computer. It is already implementet by the class <code class="docutils literal notranslate"><span class="pre">LinearRegressionNewsvendor</span></code> from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. So let’s initialize the
model and fit it to the data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>temp = [[10],[20],[30]]
demand = [5,8,10]
mdl = LinearRegressionNewsvendor(cu=15,co=5)
mdl.fit(X=temp,y=demand)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
LinearRegressionNewsvendor(co=5, cu=15)
</pre></div></div>
</div>
<p>Once the model is fitted, we can access the intercept term <span class="math notranslate nohighlight">\(b\)</span> via the argument <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> and the weight <span class="math notranslate nohighlight">\(w_1\)</span> via the argument <code class="docutils literal notranslate"><span class="pre">feature_weights_</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&quot;b: &quot;,mdl.intercept_)
print(&quot;w_1: &quot;,mdl.feature_weights_)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
b:  [2.0]
w_1:  [[0.3]]
</pre></div></div>
</div>
<p>Now we know that the linear function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> that fits best on our data is given by:</p>
<p><span class="math">\begin{equation}
q(temp)=2+0.3*temp
\end{equation}</span></p>
<p>In the next step we want to know how many steaks to defrost for the tomorrow. We check the weather forecast, which tells us that it will be 25 degrees. Given the temperature, the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> then lets us that the optimal inventory stock is: <span class="math notranslate nohighlight">\(2+0.3*25=9.5\)</span>. Instead of determining the decision ourselves, we can also use the model’s predict method:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>mdl.predict([[25]])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[9.5]])
</pre></div></div>
</div>
<p>Of course, the Yaz dataset provides a lot more demand features than just the temperature. Conseqently, we are looking for a function of the form:</p>
<p><span class="math">\begin{equation}
q(x)=b+w_1*x_1+...+w_m*x_m=b+\sum_{j=1}^{m}w_j*x_j,
\tag{7}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(x_j\)</span> represents the value of the <span class="math notranslate nohighlight">\(j\)</span>-th feature from samle <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(w_j\)</span> is the corresponding feature weight. As a result, the optimization problem that we want to solve becomes:</p>
<p><span class="math">\begin{equation}
\min_{w_0,..,w_m} \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-b-\sum_{j=1}^{m}w_j*x_{i,j})^+ + co(b+\sum_{j=1}^{m}w_j*x_{i,j}-d_i)^+\bigl],
\tag{8}
\end{equation}</span></p>
<p>In simple terms, we now have to learn the value for intercept term <span class="math notranslate nohighlight">\(b\)</span> as well as the weight for each feature <span class="math notranslate nohighlight">\(w_1...,w_m\)</span>. To make things simple, we again use the class <code class="docutils literal notranslate"><span class="pre">LinearRegressionNewsvendor</span></code>. We fit the model to the training data and use the score function to determine the average costs on the training set.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>LRN = LinearRegressionNewsvendor(cu=15,co=5)
LRN.fit(X_train,y_train[&quot;STEAK&quot;])
LRN_score = LRN.score(X_test,y_test[&quot;STEAK&quot;])
print(&quot;Avg. cost LRN: &quot;, LRN_score)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Avg. cost LRN:  -48.932357245389476
</pre></div></div>
</div>
<p>Again, we were able to reduce the average cost by taking the feature information into account compared to SAA.</p>
</div>
<div class="section" id="Deep-Learning">
<h4>Deep Learning<a class="headerlink" href="#Deep-Learning" title="Permalink to this headline">¶</a></h4>
<p>In the previouse section, we assumed that the demand is a linear combination of features. Consequently, to find the perfect mapping from features <span class="math notranslate nohighlight">\(x\)</span> to decision <span class="math notranslate nohighlight">\(q\)</span>, we specified that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> belongs to the class of linear decision functions. But sometimes there may not be a linear relationship and we need a more complex function to fit our data. One way to do this is by using a neural network (NN). Let us therefore have a look at a simple NN:</p>
<p><img alt="c2d68c8a0e544500b9cba63c5629f633" src="https://drive.google.com/uc?export=view&amp;id=1rmgdo9urd4Qx5MQrPu4sO9sto8_ogCm1" /></p>
<p>A neural network uses a cascade of many layers to obtain an output given some input data. In general it can be distingushed between input-, hidden-, and output-layer, each consisting of a number of neurons. In the first layer, the number of neurons corresoponds to the number of inputs. In other words, each neuron takes a single feature, e.g. the temperature or the weekday. The input-layer is followed by a number of hidden-layers, each consisting of an arbitary number of neurons. In the
output-layer the number of neurons correspons to the number of outputs. In our example, we only have a single neuron that outputs the decision <span class="math notranslate nohighlight">\(q\)</span> conditional on the features temperature and weekday. The individual neurons of a layer are each connected to the neurons of the layer before and behind. In a graph, the neurons can be represented as nodes and their connections as weighted edges. Let’s now have a closer look at a neuron. A neuron takes as input the outputs of the neurons from the
previous layer. Subsequently, it computes the weighted sum of inputs and adds a bias on it. More formally:</p>
<p><span class="math">\begin{equation}
bias+\sum_{i=1}^{n}x_i*w_i
\tag{9}
\end{equation}</span></p>
<p>Does this look familiar? This is the exactly the decision function <span class="math notranslate nohighlight">\((7)\)</span> that we used in the regression based model before. The only difference is that we use the term “bias” for the constant value instead of “intercept”. But what does this mean? If we were just to combine a set of linear functions, we would get a single linear function as a result. In other words, there would be no difference compared to the regression model. This is where the activation function comes into play. The
activation function is a non-linear function that transforms the weighted sum and then outputs the final result. For example, the Rectified Linear Unit (ReLU) activation function, outpus <span class="math notranslate nohighlight">\(0\)</span> if the input value is negative and otherwise its input. Thus, the neural network models a piecewise linear function, which looks like this:</p>
<p><img alt="dee552260cce4462949cbbcd4a081445" class="no-scaled-link" src="https://drive.google.com/uc?export=view&amp;id=18YRxB6jYlqV97FaEPXf-IcRuJt1nWkWI" style="width: 350px;" /></p>
<p>The goal of the network is now to find the function that fits the data best. In the regression based model the problem therefore was to learn the optimal values for the feature weights and the intercept term. This is basicly the same here. We just have a lot more weights and intercept terms (biases). Since we are trying to obtain cost-optimal decisions, the network tries to determine the weights in a way that minimizes the average cost on our data. Thus, the problem can be stated as follows:</p>
<p><span class="math">\begin{equation}
\min_{w,b} \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-\theta(x_i;w,b))^+ + co(\theta(x_i;w,b)-d_i)^+\bigl],
\tag{10}
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(\theta(\cdot)\)</span> represents the function of the network with weights <span class="math notranslate nohighlight">\(w\)</span> and biases <span class="math notranslate nohighlight">\(b\)</span>. Now look, this is again similar to equation <span class="math notranslate nohighlight">\((6)\)</span>. The only difference is that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is represented by a NN.</p>
<p>Let us now apply this approach on our Yaz dataset. We can therefore use the class <code class="docutils literal notranslate"><span class="pre">DeepLearningNewsvendor</span></code> from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. As before, we first initialize the model and fit it to the training data. Subsequently we calculate the average cost on the test set by using the score function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[58]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>DLN = DeepLearningNewsvendor(cu=15,co=5)
DLN.fit(X_train,y_train[&quot;STEAK&quot;])
DLN_score = DLN.score(X_test,y_test[&quot;STEAK&quot;])
print(&quot;Avg. cost DLN: &quot;, DLN_score)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Avg. cost DLN:  -50.968472254903695
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="id1">
<h3>Summary<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>In this part of the tutorial we have seen three differnet data driven approaches to prescribe decisions directly from data. Let us now summarize what we have learned:</p>
<ul class="simple">
<li><p>In the simplest case where we only have past demand observations we can use sample average approximation (SAA) to solve the newsvendor problem. The goal of SAA is to find the decision q that minimizes the average costs on past demand samples.</p></li>
<li><p>However, we have seen that additional demand features can improve decision making since they usually reduce the degree of uncertainty.</p></li>
<li><p>With weighted sample average approximation (wSAA) and empirical risk minimization (ERM) we have then seen two data driven approaches that can take such features into account by using a function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> that maps from features <span class="math notranslate nohighlight">\(x\)</span> to a decision <span class="math notranslate nohighlight">\(q\)</span>.</p></li>
<li><p>WSAA defines the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> point-wise. It is based on deriving sample weights from features and optimizing SAA against a re-weighting of the training data. To determine the weights we can use different weight functions e.g. based on k-nearest-neighbor regression,regression trees or random forest regression.</p></li>
<li><p>The ERM approach, on the other hand, tries to find the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> that maps directly from features to a decision by minimizing its empirical risk. Therefore, we have to specify the function class to which the decision function belongs. With the classical ERM newsvendor, and the deep learning newsvendor we have seen two methods that define <span class="math notranslate nohighlight">\(q(\cdot)\)</span> in a different manner. While the former one assumes that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is a linear decision function given by the
weighted sum of features, the later one defines <span class="math notranslate nohighlight">\(q(\cdot)\)</span> as a deep neural network</p></li>
</ul>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../tutorial.html" class="btn btn-neutral float-left" title="Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Andreas Philippi

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>