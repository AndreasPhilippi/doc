

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>The Data Driven Newsvendor &mdash; ddop  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> ddop
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Tutorial</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ddop</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>The Data Driven Newsvendor</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/tutorial_modules/E_Learning (1).ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container,
div.nbinput.container div.prompt,
div.nbinput.container div.input_area,
div.nbinput.container div[class*=highlight],
div.nbinput.container div[class*=highlight] pre,
div.nboutput.container,
div.nboutput.container div.prompt,
div.nboutput.container div.output_area,
div.nboutput.container div[class*=highlight],
div.nboutput.container div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    background: #f5f5f5;
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="The-Data-Driven-Newsvendor">
<h1>The Data Driven Newsvendor<a class="headerlink" href="#The-Data-Driven-Newsvendor" title="Permalink to this headline">¶</a></h1>
<p>The newsvendor problem is a classical single period inventory problem, which involves making decisions on order quantities under uncertainty in demand. The traditional way to solve the problem assumes knowledge of the probability distribution of demand. However, in practice, the decision maker does not know the true demand distribution.</p>
<p>In this tutorial you will learn how to solve the newsvendor problem when the underlying demand distribution is unknown but the decision maker has access to historical demand observations. For this purpose the tutorial is structured as follows: - In the first part, we will recap the classical newsvendor problem - In the second part, we will see how to solve the problem by applying a parametric approach - In the third part we will introduce different data-driven approaches to solve the newsvendor
problem.</p>
<p>Moreover, you will learn how to use our <code class="docutils literal notranslate"><span class="pre">ddop</span></code> python API to apply the different approaches on a real world dataset.</p>
<div class="section" id="Example">
<h2>Example<a class="headerlink" href="#Example" title="Permalink to this headline">¶</a></h2>
<p>To illustrate the problem in a meaningful way we will use the real world problem of YAZ within this tutorial. YAZ is a fast casual restaurant in Stuttgart providing great oriental cuisine. The main ingredients for the meals at YAZ, e.g. steak, lamb, fish etc. are prepared at a central factory and are deep-frozen to achieve longer shelf lives. Depending on the estimated demand for the next day, the restaurant manager has to decide how much of the ingredients to defrost over night. These defrosted
ingredients/meals then have to be sold within the following day. If the defrosted quantity was to low to meet the demand this will incur underage cost of <span class="math notranslate nohighlight">\(cu\)</span>. On the other hand, if the quantity was to high, unsold ingredients have to be disposed which in turn will lead to overage cost of <span class="math notranslate nohighlight">\(co\)</span>. Therefore, the store manager wants to choose the order quantity that minimizes the sum of the expected costs.</p>
<p><img alt="image1" src="https://drive.google.com/uc?export=view&amp;id=1UPRyUC56wMVd554iHsvOks-MBJaXYwMf" /></p>
</div>
<div class="section" id="Getting-Started">
<h2>Getting Started<a class="headerlink" href="#Getting-Started" title="Permalink to this headline">¶</a></h2>
<p>Before we are ready to start we have to load the libraries we will need in the following.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from ddop.datasets import load_yaz
from ddop.metrics.costs import calc_avg_costs
from ddop.newsvendor import SampleAverageApproximationNewsvendor
from ddop.newsvendor import RandomForestWeightedNewsvendor
from ddop.newsvendor import KNeighborsWeightedNewsvendor
from ddop.newsvendor import EmpiricalRiskMinimizationNewsvendor
from ddop.newsvendor import DeepLearningNewsvendor
from matplotlib import pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import scipy
from scipy.stats import norm
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
Using TensorFlow backend.
/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
  import pandas.util.testing as tm
</pre></div></div>
</div>
<p>You can install missing packages by uncommenting and executing the following cell:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>pip install tensorflow==2.1.0 scikit-learn==0.23.0 keras==2.3.1 ddop==0.5.3 seaborn==0.10.1 matplotlib
</pre></div>
</div>
</div>
</div>
<div class="section" id="Data">
<h2>Data<a class="headerlink" href="#Data" title="Permalink to this headline">¶</a></h2>
<p>As mentioned before, we we will consider the real world problem of Yaz within this tutorial. Therefore, we will use the Yaz dataset that is included in <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. The dataset stores the demand data for calamari, fish, shrimp, chicken, koefte, lamb and steak over 760 days. In addition, it comes with a bunch of different demand features that may have predictive power. These features include information about the day, month, year, weather conditions and more. You can load the dataset using the
<code class="docutils literal notranslate"><span class="pre">load_yaz</span></code> function from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>yaz = load_yaz(encode_date_features=True)
</pre></div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">load_yaz</span></code> will return an object containing the feature data, the target variables as well as the whole dataframe. You can access the dataframe using the <code class="docutils literal notranslate"><span class="pre">frame</span></code> attribute:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>df = yaz.frame
df
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>ISHOLIDAY</th>
      <th>WEEKEND</th>
      <th>WIND</th>
      <th>CLOUDS</th>
      <th>RAINFALL</th>
      <th>HOURS_OF_SUNSHINE</th>
      <th>TEMPERATURE</th>
      <th>WEEKDAY_FRI</th>
      <th>WEEKDAY_MON</th>
      <th>WEEKDAY_SAT</th>
      <th>WEEKDAY_SUN</th>
      <th>WEEKDAY_THU</th>
      <th>WEEKDAY_TUE</th>
      <th>WEEKDAY_WED</th>
      <th>MONTH_APR</th>
      <th>MONTH_AUG</th>
      <th>MONTH_DEC</th>
      <th>MONTH_FEB</th>
      <th>MONTH_JAN</th>
      <th>MONTH_JUL</th>
      <th>MONTH_JUN</th>
      <th>MONTH_MAR</th>
      <th>MONTH_MAY</th>
      <th>MONTH_NOV</th>
      <th>MONTH_OCT</th>
      <th>MONTH_SEP</th>
      <th>YEAR_2013</th>
      <th>YEAR_2014</th>
      <th>YEAR_2015</th>
      <th>CALAMARI</th>
      <th>FISH</th>
      <th>SHRIMP</th>
      <th>CHICKEN</th>
      <th>KOEFTE</th>
      <th>LAMB</th>
      <th>STEAK</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>1.9</td>
      <td>7.7</td>
      <td>0.1</td>
      <td>150</td>
      <td>15.9</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>6</td>
      <td>12</td>
      <td>40</td>
      <td>23</td>
      <td>50</td>
      <td>36</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>1</td>
      <td>2.7</td>
      <td>6.9</td>
      <td>10.7</td>
      <td>0</td>
      <td>13.2</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>8</td>
      <td>8</td>
      <td>5</td>
      <td>44</td>
      <td>36</td>
      <td>37</td>
      <td>30</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>1</td>
      <td>1.4</td>
      <td>8.0</td>
      <td>0.4</td>
      <td>0</td>
      <td>10.6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>5</td>
      <td>11</td>
      <td>19</td>
      <td>12</td>
      <td>22</td>
      <td>16</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>2.3</td>
      <td>6.4</td>
      <td>0.0</td>
      <td>176</td>
      <td>13.3</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>4</td>
      <td>4</td>
      <td>2</td>
      <td>28</td>
      <td>13</td>
      <td>28</td>
      <td>22</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>1.7</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>13.5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>7</td>
      <td>4</td>
      <td>9</td>
      <td>22</td>
      <td>18</td>
      <td>29</td>
      <td>29</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>755</th>
      <td>0</td>
      <td>0</td>
      <td>1.6</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>3.5</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
      <td>8</td>
      <td>47</td>
      <td>30</td>
      <td>32</td>
      <td>32</td>
    </tr>
    <tr>
      <th>756</th>
      <td>0</td>
      <td>0</td>
      <td>1.8</td>
      <td>2.2</td>
      <td>0.0</td>
      <td>362</td>
      <td>14.6</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>1</td>
      <td>13</td>
      <td>38</td>
      <td>31</td>
      <td>14</td>
      <td>38</td>
    </tr>
    <tr>
      <th>757</th>
      <td>0</td>
      <td>0</td>
      <td>1.8</td>
      <td>0.7</td>
      <td>0.0</td>
      <td>405</td>
      <td>14.7</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>8</td>
      <td>11</td>
      <td>47</td>
      <td>32</td>
      <td>42</td>
      <td>24</td>
    </tr>
    <tr>
      <th>758</th>
      <td>0</td>
      <td>0</td>
      <td>1.9</td>
      <td>6.9</td>
      <td>0.0</td>
      <td>44</td>
      <td>16.0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>3</td>
      <td>6</td>
      <td>9</td>
      <td>50</td>
      <td>40</td>
      <td>44</td>
      <td>32</td>
    </tr>
    <tr>
      <th>759</th>
      <td>0</td>
      <td>1</td>
      <td>1.9</td>
      <td>5.6</td>
      <td>0.0</td>
      <td>46</td>
      <td>17.3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>2</td>
      <td>2</td>
      <td>45</td>
      <td>25</td>
      <td>6</td>
      <td>20</td>
    </tr>
  </tbody>
</table>
<p>760 rows × 36 columns</p>
</div></div>
</div>
<p>Similarly, you can access the feature data using the <code class="docutils literal notranslate"><span class="pre">data</span></code> attribute and the targets with the <code class="docutils literal notranslate"><span class="pre">target</span></code> attribute.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># the feature data
X = yaz.data

# the target data
y = yaz.target
</pre></div>
</div>
</div>
<p>As we want to built and compare different models in the course of this tutorial, we have to split the data into train- and test set. While we use the training set to build our model, we need the test set to evaluate it on unknown data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, shuffle=False)
</pre></div>
</div>
</div>
<p>Since some models that we are going to consider are sensitive to the variance in the data we have to normalize our features. We therefore use the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> from sklearn. We fit the scaler on the train set and transform both, train- and test data.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
</pre></div>
</div>
</div>
</div>
<div class="section" id="The-classical-Newsvendor-Problem">
<h2>The classical Newsvendor Problem<a class="headerlink" href="#The-classical-Newsvendor-Problem" title="Permalink to this headline">¶</a></h2>
<p>Now that we are ready to start, let us first recap the newsvendor problem: The newsvendor problem is a single period inventory problem that refers to a situation in which a seller has to determine the order quantity of perishable goods for the next selling period, after which left-over stock becomes worthless. Hence, at the end of the selling period each unit of unsold stock then incurs overage costs of <span class="math notranslate nohighlight">\(co\)</span> and each unit of demand that cannot be satisfied incurs underage costs of
<span class="math notranslate nohighlight">\(cu\)</span>. Therefore, the newsvendor wants to choose the order quantity that minimizes the sum of the expected costs mentioned above.</p>
<p>Given this background, the optimal order quantity for the newsvendor problem can be formulated as follows:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id1"><span class="problematic" id="id2">`</span></a>begin{equation}</dt><dd><p>min_{qgeq 0} = E_D[cu(D-q)^+ + co(q-D)^+],</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the random future demand, <span class="math notranslate nohighlight">\(q\)</span> is the order quantity, <span class="math notranslate nohighlight">\(cu\)</span> and <span class="math notranslate nohighlight">\(co\)</span> are the per-unit under- and overage costs and <span class="math notranslate nohighlight">\((\cdot)^+ := \max\{0,\cdot\}\)</span>. If the demand distribution is known, then the optimal decision can be obtained as:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id3"><span class="problematic" id="id4">`</span></a>begin{equation}</dt><dd><p>q^*=F^{-1}biggl(frac{cu}{cu+co}biggl)=F^{-1}(alpha),</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>where <span class="math notranslate nohighlight">\(F^{-1}(\cdot)\)</span> is the inverse cumulative density function of the demand distribution.</p>
</div>
<div class="section" id="Parametric-Approach">
<h2>Parametric Approach<a class="headerlink" href="#Parametric-Approach" title="Permalink to this headline">¶</a></h2>
<p>Now let us go back to our example of YAZ where we want to determine how many ingredients to defrost overnight. If we knew the probability distribution for demand we could simply solve the initial optimization problem. Unfortunately, we don’t know this probability distribution. However, we have access to historical demand samples to which we can fit a parametric distribution. So let’s do this for a single product e.g. steak. We start by exploring the data with a histogram. For plotting we are
useing <code class="docutils literal notranslate"><span class="pre">seaborn</span></code> - a Python data visualization libary.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># get the demand samples for steak
steak_demand = y_train[&quot;STEAK&quot;]

# print a histogram
sns.distplot(steak_demand, hist=True, norm_hist=False, kde=False,
             hist_kws={&#39;edgecolor&#39;:&#39;black&#39;})

plt.ylabel(&#39;Frequnecy&#39;)
plt.xlabel(&#39;Demand&#39;)

plt.show(sns)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_modules_E_Learning_(1)_22_0.png" src="../_images/tutorial_modules_E_Learning_(1)_22_0.png" />
</div>
</div>
<p>This reminds of a normal distribution, doesn’t it? So let us fit a normal distribution to our data by estimation the parameters <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span>:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine mean and standard deviation
mean = round(y_train[&quot;STEAK&quot;].mean(),2)
std = round(y_train[&quot;STEAK&quot;].std(),2)

# plot histogram and fit a normal distribution to it
sns.distplot(steak_demand, hist=True, fit=scipy.stats.norm, norm_hist=False,
             kde=False, hist_kws={&#39;edgecolor&#39;:&#39;black&#39;})

plt.ylabel(&#39;Empirical probability&#39;)
plt.xlabel(&#39;Demand&#39;)
plt.title(&quot;Fit result: mean = %.2f,  std = %.2f&quot; % (mean, std))

plt.show(sns)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_modules_E_Learning_(1)_24_0.png" src="../_images/tutorial_modules_E_Learning_(1)_24_0.png" />
</div>
</div>
<p>Before we can apply the newsvendor problem we have to define the under- and overage costs for chicken. We assume each unit of unsold steak costs Yaz 5€, and each unit of demand that cannot be met because Yaz has run out of inventory costs 15€.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>cu = 15
co = 5
</pre></div>
</div>
</div>
<p>Using this information, the newsvendor problem then tells us how much steak to defrost overnight.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine optimal inventory quantity
q = round(norm(mean, std).ppf(cu/(cu+co)))
q
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
30.0
</pre></div></div>
</div>
<p>We call this the traditional parametric approach  we first assume the demand falls in a family of parametric distributions, estimate its parameters, and then solve the initial optimization problem.</p>
<p>To see how good this approach works we can calculate the average costs on the training set associated with our decision. Therefore, we can use the <code class="docutils literal notranslate"><span class="pre">calc_avg_costs</span></code> function implemented in <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. The function takes four arguments in the following order - the true values, the predicted values, the underage costs and the overage costs.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># create array with lenght of the test set and inventory quantity q
y_pred = np.full(y_test[&quot;STEAK&quot;].shape[0],q)

# calculate and print average costs
avg_costs = calc_avg_costs(y_test[&quot;STEAK&quot;], y_pred, cu, co)
print(&quot;Average costs: &quot;+str(avg_costs))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Average costs: [-65.60526316]
</pre></div></div>
</div>
<p>As you can see we have average cost of 65.61€ associated with our decision. Now the question is whether we can do better. To answer this question let us go back to our data. So far we only used historical steak demand samples. However, we have access to exogenous features that we expect to be correlated to the demand. For example, we know that on average Yaz sells the most steak on Fridays and Saturdays and the least on Sundays and Mondays.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>days = [&#39;MON&#39;,&#39;TUE&#39;,&#39;WED&#39;,&#39;THU&#39;,&#39;FRI&#39;,&#39;SAT&#39;,&#39;SUN&#39;]
mean_per_day = []
for day in days:
  data = pd.concat([X_train, y_train], axis=1)
  mean = data[data[&quot;WEEKDAY_&quot;+day] == 1][&quot;STEAK&quot;].mean()
  mean_per_day.append(mean)
  print(day,&quot;: &quot;,round(mean,2))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
MON :  18.79
TUE :  20.27
WED :  21.8
THU :  21.89
FRI :  26.43
SAT :  37.8
SUN :  16.55
</pre></div></div>
</div>
<p>One way to take this information into consideration would be by fitting a normal distribution for each weekday.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>std_per_day = []
for day in days:
  data = pd.concat([X_train, y_train], axis=1)
  std = data[data[&quot;WEEKDAY_&quot;+day] == 1][&quot;STEAK&quot;].std()
  std_per_day.append(std)
  print(day,&quot;: &quot;,round(std,2))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
MON :  7.44
TUE :  6.33
WED :  6.54
THU :  6.79
FRI :  8.06
SAT :  12.21
SUN :  6.08
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># plot
fig, ((ax1, ax2, ax3), (ax4, ax5, ax6),
      (ax7, ax8, ax9)) = plt.subplots(3, 3, figsize=(12,12))

for i in range(9):
  if i&lt;=6:
    steak_demand_day = df[df[&#39;WEEKDAY_&#39;+days[i]]==1][&#39;STEAK&#39;]

    sns.distplot(steak_demand_day, ax=fig.axes[i],
                 fit=scipy.stats.norm,kde=False, hist_kws={&#39;edgecolor&#39;:&#39;black&#39;})
    fig.axes[i].set_title(&quot;%s: mean = %.2f,  std = %.2f&quot; % (days[i], mean_per_day[i], std_per_day[i]))

  else:
    fig.axes[i].set_frame_on(False)
    fig.axes[i].get_yaxis().set_visible(False)

  fig.axes[i].set_ylabel(&#39;Empirical probability&#39;)
  fig.axes[i].set_xlabel(&#39;Demand&#39;)
  fig.axes[i].label_outer()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/tutorial_modules_E_Learning_(1)_35_0.png" src="../_images/tutorial_modules_E_Learning_(1)_35_0.png" />
</div>
</div>
<p>Given these distributions, we can write a simple predict function that solves the newsvendor problem depending on the weekday:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def predict(X):
  &quot;&quot;&quot;
  For each row in X, check which day of the week it is, get the mean and
  standard deviation of the corresponding  distribution and then calculate
  the optimal inventory quantity.

  Parameters
  ----------
  X : Pandas DataFrame of shape (n_samples, n_features)
      The input samples to predict.

  Returns
  ----------
  y : array of shape (n_samples,)
      The predicted values
  &quot;&quot;&quot;

  pred = []
  for index, row in X.iterrows():
    if row[&#39;WEEKDAY_MON&#39;]==1:
      mean = mean_per_day[0]
      std = std_per_day[0]
    elif row[&#39;WEEKDAY_TUE&#39;]==1:
      mean = mean_per_day[1]
      std = std_per_day[1]
    elif row[&#39;WEEKDAY_WED&#39;]==1:
      mean = mean_per_day[2]
      std = std_per_day[2]
    elif row[&#39;WEEKDAY_THU&#39;]==1:
      mean = mean_per_day[3]
      std = std_per_day[3]
    elif row[&#39;WEEKDAY_FRI&#39;]==1:
      mean = mean_per_day[4]
      std = std_per_day[4]
    elif row[&#39;WEEKDAY_SAT&#39;]==1:
      mean = mean_per_day[5]
      std = std_per_day[5]
    else:
      mean = mean_per_day[6]
      std = std_per_day[6]

    q = round(norm(mean, std).ppf(cu/(cu+co)))
    pred.append(q)

  return np.array(pred)
</pre></div>
</div>
</div>
<p>Then we apply the function to the test data and calculate the average costs again.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># predict optimal inventory quantity
y_pred = predict(X_test)

# calculate and print average costs
avg_costs = calc_avg_costs(y_test[&quot;STEAK&quot;], y_pred, cu, co)
print(&quot;Average costs: &quot;+str(avg_costs))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Average costs: [-54.07894737]
</pre></div></div>
</div>
<p>Now look  we were able to reduce the average cost from 65.61€ to 54.08€. That is a great improvment! But maybe we can do even better by considering the other features of the dataset as well. To do this, we can fit a distribution for samples with the same features - like we did before but now for more features than just the weekday. But this has two main drawbacks: 1. Let us say we just have two features, the weekday and the month. If we would like to fit a distribution for each feature
combination we would have to fit 7*12=84 distributions. This sounds like a lot of work, doesn’t it? 2. Now consider the case where it is monday and january. As you can see below we only have 8 samples with the same feature attributes. Such a small number of samples makes it hard to fit a meaningful distribution. Moreover, with increasing number of features we may not have a single sample with the same values.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>X_train[(X_train[&#39;WEEKDAY_MON&#39;]==1)&amp;(X_train[&#39;MONTH_JAN&#39;]==1)].shape[0]
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
8
</pre></div></div>
</div>
<p>Instead of fitting a distribution for samples with the same features, we can fit a machine-learning model to predict the demand. For example we can use the <code class="docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># train a decision tree regressor
mdl = DecisionTreeRegressor(max_depth=5, random_state=1)
mdl.fit(X_train,y_train[&quot;STEAK&quot;])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
DecisionTreeRegressor(max_depth=5, random_state=1)
</pre></div></div>
</div>
<p>Of course we cannot assume the model to be perfect: first because of the model error itself, and second, because of the uncertainty in demand. For this reason, we need to adjust the predictions for uncertainty to obtain optimal decisions. We can get a representation of the remaining uncertainty by evaluating the distribution of the prediction error on the training data. Assuming the error to be distributed normal, we estimate its parameters <span class="math notranslate nohighlight">\(\mu_{e}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{e}\)</span>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># predict on training data
train_predicitons = mdl.predict(X_train)

# estimate mean and standard deviation of the model error
error = y_train[&quot;STEAK&quot;]-train_predicitons
error_mean = error.mean()
error_std = error.std()

</pre></div>
</div>
</div>
<p>We then pass the error distribution to the initial optimization problem to determine an additional safety stock. The final order decision is then the sum of both the prediction generated by our model and the safety stock. More formally, the newsvendor problem can then be described as:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id5"><span class="problematic" id="id6">`</span></a>begin{equation}</dt><dd><p>q(x)^{*} = mu(x)+Phi^{-1}(alpha),</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>where <span class="math notranslate nohighlight">\(\mu(\cdot)\)</span> is the function of the machine-learning model that maps from the feature vector <span class="math notranslate nohighlight">\(x\)</span> to demand level, and <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> the inverse cdf of the error distribution with mean <span class="math notranslate nohighlight">\(\mu_{e}\)</span> and <span class="math notranslate nohighlight">\(\sigma_{e}\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span># determine safety buffer
safety_buffer = norm(error_mean, error_std).ppf(cu/(cu+co))

# predict test data
pred = mdl.predict(X_test)

# add safety buffer prediction from model
pred = pred + safety_buffer

# calculate and print average costs
avg_costs = calc_avg_costs(y_test[&quot;STEAK&quot;], pred, cu, co)
print(&quot;Average costs: &quot;+str(avg_costs))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Average costs: [-45.5082525]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>safety_buffer
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
4.033345941513752
</pre></div></div>
</div>
<p>Again, we were able to reduce our average cost from 51.97€ to 45.51€.</p>
<div class="section" id="Summary">
<h3>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¶</a></h3>
<p>Let us now summarize what we have learned so far: - In case we don’t know the demand distribution, but have access to historical demand observations, we can fit a distribution to the historical samples and then solve the initial problem. - We can improve our inventory decision by taking into account features. To do this, we can fit a distribution for samples with the same features. However, this does not work well for a big number of features. - Instead, we can fit a machine-learning model to
predict demand and, given its error distribution, calculate an additional safety stock.</p>
</div>
</div>
<div class="section" id="Data-Driven-Approach">
<h2>Data Driven Approach<a class="headerlink" href="#Data-Driven-Approach" title="Permalink to this headline">¶</a></h2>
<p>In this part of the tutorial, we introduce different <strong>“data driven”</strong> approaches to solve the newsvendor problem. In contrast to the traditional way of first estimating the demand and then solving the initial optimization problem, these approches directly prescribes decisions from data.</p>
<div class="section" id="Sample-Average-Approximation">
<h3>Sample Average Approximation<a class="headerlink" href="#Sample-Average-Approximation" title="Permalink to this headline">¶</a></h3>
<p>In the case, where only historical demand data is available, a sensible data-driven approach is to substitute the true expectation with a sample average expectations and solve the resulting optimization problem:</p>
<p><span class="math">\begin{equation}q^{*}=\min _{q \geq 0} \frac{1}{n} \sum_{i=1}^{n}\left[c u\left(d_{i}-q\right)^{+}+c o\left(q-d_{i}\right)^{+}\right]
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the total number of samples and <span class="math notranslate nohighlight">\(d_i\)</span> is the ith demand observation. This approach is called <strong>sample average approximation (SAA)</strong>. To fully understand how this approach works, lets go through a simple example.</p>
<p><strong>Example:</strong> Determine how many steaks to defrost applying <strong>SAA</strong>. The historic demand for steak is given by: <span class="math notranslate nohighlight">\(D=[27,29,30]\)</span>. You can sell steak in your restaurant for 15€ (underage costs). Unsold units incur overage costs of 5€.</p>
<p>Let’s now try to minimize the optimization problem:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id7"><span class="problematic" id="id8">`</span></a>begin{equation}</dt><dd><p>…..\
q=27: frac{1}{3}Bigl[15*(27-27)+15*(29-27)+15*(30-27)Bigl]=frac{1}{3}(15*0+15*2+5*3)=25\
q=28: frac{1}{3}Bigl[5*(28-27)+15*(29-28)+15*(30-28)Bigl]=frac{1}{3}(5*1+15*1+15*2)=16,67\
q=29: frac{1}{3}Bigl[5*(29-27)+15*(29-29)+15*(30-29)Bigl]=frac{1}{3}(5*2+15*0+15*1)=8,33\
q=30: frac{1}{3}Bigl[5*(30-27)+5*(30-29)+15*(30-30)Bigl]=frac{1}{3}(5*3+5*1+15*0)=6,67\
q=31: frac{1}{3}Bigl[5*(31-27)+5*(31-29)+5*(31-30)Bigl]=frac{1}{3}(5*4+5*2+5*1)=11,67\
…..</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>As you can see, we are trying to find the decision <span class="math notranslate nohighlight">\(q\)</span> that minimizes the average costs on our historical samples. Consequently, in this case the optimal decision is given by <span class="math notranslate nohighlight">\(q=30\)</span>.</p>
<p>Now that we know how SAA works let us apply the approach on our dataset. We can therefore use the <code class="docutils literal notranslate"><span class="pre">SampleAverageApproximationNewsvendor</span></code> class from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. To initialize the model we just have to pass the under- and overage costs. Then we can fit the model to the historical demand samples and predict the optimal decision.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>SAA = SampleAverageApproximationNewsvendor(cu,co)
SAA.fit(y_train[&quot;STEAK&quot;])
SAA.predict()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[28]])
</pre></div></div>
</div>
<p>We see, the optimal decision is 28. To calculate the average cost on the training data, we can use the model’s score function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>SAA.score(y_test[&quot;STEAK&quot;])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([-60.23684211])
</pre></div></div>
</div>
</div>
<div class="section" id="Weighted-Sample-Average-Approximation">
<h3>Weighted Sample Average Approximation<a class="headerlink" href="#Weighted-Sample-Average-Approximation" title="Permalink to this headline">¶</a></h3>
<p>Eventhough SAA is a common and effective approach, it only considers historical demand data. However, in the first part of the tutorial we have seen that demand depends on exogenous factors such as the day. As first step, we therefore fitted a distribution for each weekday. We can now do the same here by fitting a SAA model for each weekday. This means that we are trying to find the decision <span class="math notranslate nohighlight">\(q\)</span> that minimizes the average cost for only samples with the same day instead of trying to find
the decision <span class="math notranslate nohighlight">\(q\)</span> that minimizes the average cost for all historical samples.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>days = [&#39;MON&#39;,&#39;TUE&#39;,&#39;WED&#39;,&#39;THU&#39;,&#39;FRI&#39;,&#39;SAT&#39;,&#39;SUN&#39;]

pred = []

SAA = SampleAverageApproximationNewsvendor(cu,co)

for day in days:
  steak_demand_day = df[df[&#39;WEEKDAY_&#39;+day]==1][&#39;STEAK&#39;]
  SAA.fit(steak_demand_day)
  pred.append(SAA.predict().item(0))

print(pred)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[21, 23, 26, 25, 30, 44, 21]
</pre></div></div>
</div>
<p>Now ginve the optimla decision for each day, we can write a new prediction function.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>def predict(X):
  &quot;&quot;&quot;
  Get the prediction for each sample in X depending on the weekday.

  Parameters
  ----------
  X : Pandas DataFrame of shape (n_samples, n_features)
      The input samples to predict.

  Returns
  ----------
  y : array of shape (n_samples,)
      The predicted values
  &quot;&quot;&quot;

  pred = []
  for index, row in X.iterrows():
    if row[&quot;WEEKDAY_MON&quot;]==1:
      pred.append(21)
    elif row[&quot;WEEKDAY_TUE&quot;]==1:
      pred.append(23)
    elif row[&quot;WEEKDAY_WED&quot;]==1:
      pred.append(26)
    elif row[&quot;WEEKDAY_THU&quot;]==1:
      pred.append(25)
    elif row[&quot;WEEKDAY_FRI&quot;]==1:
      pred.append(30)
    elif row[&quot;WEEKDAY_SAT&quot;]==1:
      pred.append(44)
    else:
      pred.append(21)

  return np.array(pred)
</pre></div>
</div>
</div>
<p>We can then apply the function on the test set and calculate the average costs:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>pred = predict(X_test)
calc_avg_costs(y_test[&quot;STEAK&quot;],pred,cu,co)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([-50.5])
</pre></div></div>
</div>
<p>Look  again we were able to reduce the average cost from 60.24€ to 50.50€ by taking into account the weekday. We could now go a step further, by fitting a model for samples with the same features. However, as we have seen in part 1, this is not the best way. Instead, let us assign a weight to each historical observation that indicates the similarity to a new instance. To calculate the sample weights we can use machine-learning techniques - for example a regression tree. To see how we can do
this let us consider a small example.</p>
<p>Let’s say we have the following historical samples with two features (Weekend,Temperature) and we want to calculate their weights based on a new sample <span class="math notranslate nohighlight">\(x=(0,18)\)</span>.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 16%" />
<col style="width: 18%" />
<col style="width: 50%" />
<col style="width: 16%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Sample</p></th>
<th class="head"><p>Weekend</p></th>
<th class="head"><p>Temperature [in °C]</p></th>
<th class="head"><p>Demand</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0</p></td>
<td><p>19</p></td>
<td><p>27</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>1</p></td>
<td><p>25</p></td>
<td><p>29</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>1</p></td>
<td><p>23</p></td>
<td><p>30</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>0</p></td>
<td><p>25</p></td>
<td><p>18</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>0</p></td>
<td><p>24</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>0</p></td>
<td><p>22</p></td>
<td><p>23</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>0</p></td>
<td><p>11</p></td>
<td><p>21</p></td>
</tr>
</tbody>
</table>
<p>First, we fit a regression tree on our historical samples, which gives us the following result:</p>
<p><img alt="c7239c3ce52c46d7a468dc1e6e417c6f" class="no-scaled-link" src="https://drive.google.com/uc?export=view&amp;id=1fXW6PV2bUcnZSNByTde39k8hEgRB_5aV" style="width: 450px;" /></p>
<p>As we can see, the regression tree splits our data into four leafs based on the two features. In the next step we can use this tree to determine the sample weights. Therefore, we look to which leaf the new instance <span class="math notranslate nohighlight">\(x=(0,18)\)</span> belongs. In the first level of the tree we follow the right path since it is no weekend. Then, given the temperature forecast of <span class="math notranslate nohighlight">\(18°\)</span> we end up in leaf 1 together with sample 1,6 and 7, which means that these observations are most similar to <span class="math notranslate nohighlight">\(x\)</span>. Using
this information we assign a weight of <span class="math notranslate nohighlight">\(\frac{1}{3}\)</span> to each of the three samples falling into the same leaf. The sample weights are then given by:</p>
<p><span class="math">\begin{equation}
w_1=1/3,\:w_2=0,\:w_3=0,\:w_4=0,\:w_5=0,\:w_6=1/3,\:w_7=1/3
\end{equation}</span></p>
<p>Now that we have calculated the sample weights we can solve the optimization problem like we did before. The only difference now is, that we multiply each observation with its corresponding weight, which indicates the similarity to the new sample.</p>
<p><span class="math">\begin{equation}
q=18: \frac{1}{3}\bigl[15*(27-18)\bigl]+0\bigl[15*(29-18)\bigl]+...+0\bigl[15*(20-18)\bigl]+\frac{1}{3}\bigl[15*(23-18)\bigl]+\frac{1}{3}\bigl[15*(21-18)\bigl] = 85\\
......\\
q=26: \frac{1}{3}\bigl[15*(27-26)\bigl]+0\bigl[15*(29-26)\bigl]+...+0\bigl[5*(26-20)\bigl]+\frac{1}{3}\bigl[5*(26-23)\bigl]+\frac{1}{3}\bigl[5*(26-21)\bigl] = 18.33\\
q=27: \frac{1}{3}\bigl[15*(27-27)\bigl]+0\bigl[15*(29-27)\bigl]+...+0\bigl[5*(27-20)\bigl]+\frac{1}{3}\bigl[5*(27-23)\bigl]+\frac{1}{3}\bigl[5*(27-21)\bigl] = 16.67\\
q=28: \frac{1}{3}\bigl[5*(28-27)\bigl]+0\bigl[15*(29-28)\bigl]+...+0\bigl[5*(28-20)\bigl]+\frac{1}{3}\bigl[5*(28-23)\bigl]+\frac{1}{3}\bigl[5*(28-21)\bigl] = 21.67\\
......\\
q=30: \frac{1}{3}\bigl[5*(30-27)\bigl]+0\bigl[5*(30-29)\bigl]+...+0\bigl[5*(30-20)\bigl]+\frac{1}{3}\bigl[5*(30-23)\bigl]+\frac{1}{3}\bigl[5*(30-21)\bigl] = 30.67
\end{equation}</span></p>
<p>So, as we can see from our calculation, <span class="math notranslate nohighlight">\(q=27\)</span> is the quantity minimizing the average costs on the weighted samples.</p>
<p>We call this approach <strong>“Weigthed Sample Average Approximation (wSAA)”</strong> as it can be seen as weighted form of SAA. More formally the problem can be stated as follow:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id9"><span class="problematic" id="id10">`</span></a>begin{equation}</dt><dd><p>q(x)^*=min_{qgeq 0} sum_{i=1}^{n}w_i(x)bigl[cu(d_i-q)^+ + co(q-d_i)bigl],</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>where <span class="math notranslate nohighlight">\(w_i(x) \in [0,1]\)</span> is the weight of the ith sample based on a new instance <span class="math notranslate nohighlight">\(x\)</span>. The weight function <span class="math notranslate nohighlight">\(w_i(\cdot)\)</span> can be considered a similarity function. In our example above we used a weight function based on a tree regressor given by:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id11"><span class="problematic" id="id12">`</span></a>begin{equation}</dt><dd><p>w_{i}^{Tree}(x)=frac{mathbb{1}x_i in R(x,theta)}{N(x,theta)},</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>where <span class="math notranslate nohighlight">\(\mathbb{1}\)</span> is the indicator function, <span class="math notranslate nohighlight">\(x\)</span> is the feature vector of a new sample, <span class="math notranslate nohighlight">\(x_i\)</span> is the feature vector of the ith sample, <span class="math notranslate nohighlight">\(R(x,\theta)\)</span> is the leaf containing <span class="math notranslate nohighlight">\(x\)</span>, and <span class="math notranslate nohighlight">\(N(x,\theta)\)</span> is the number of samples falling in leaf <span class="math notranslate nohighlight">\(R(x,\theta)\)</span>. Moreover, <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter determining how the tree is grown. In simple terms, the function will give a wight of <span class="math notranslate nohighlight">\(\frac{1}{N(x,\theta)}\)</span> to the ith sample <span class="math notranslate nohighlight">\(x_i\)</span> if it belongs to
the same leaf as the new instance <span class="math notranslate nohighlight">\(x\)</span>. If not, a weight of <span class="math notranslate nohighlight">\(0\)</span> is assigned.</p>
<p>Obviously, the performance of a wSAA approach is driven by the choice of weight function. In the following we therefore introduce two other weight functions based on k-nearest-neighbor- and random forest regression.</p>
<p><strong>Random forest:</strong></p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id13"><span class="problematic" id="id14">`</span></a>begin{equation}</dt><dd><p>w_{i}^{RF}(x)=frac{1}{T} sum_{t=1}^{T} frac{mathbb{1}_{x_i in R(x, theta_t)}}{N(x,theta_t)},</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>where <span class="math notranslate nohighlight">\(T\)</span> is the number of trees in the forest, <span class="math notranslate nohighlight">\(R(x,\theta_t)\)</span> is the leaf of tree <span class="math notranslate nohighlight">\(t\)</span> containing <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(N(x,\theta_t)\)</span> is the number of samples falling in leaf <span class="math notranslate nohighlight">\(R(x,\theta_t)\)</span>.</p>
<p><strong>K-nearest-neighbor:</strong></p>
<p><span class="math">\begin{equation}
w_{i}^{k \mathrm{NN}}(x)=\frac{1}{k} \mathbb{1}\left[x_i \text { is a } k \mathrm{NN} \text { of } x\right],
\end{equation}</span></p>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the number of neighbors. In simple terms, the function will give a weight of <span class="math notranslate nohighlight">\(\frac{1}{k}\)</span> to sample <span class="math notranslate nohighlight">\(x_i\)</span> if it is a k-nearest-neighbor of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>We now dont want to go into more detail. Instead, let us apply wSAA on our dataset. We can therefore use the class <code class="docutils literal notranslate"><span class="pre">RandomForestWeightedNewsvendor</span></code> and <code class="docutils literal notranslate"><span class="pre">KNeighborsWeightedNewsvendor</span></code> from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. To initialize the models, we only have to pass our overage- and underage costs. However, there are more model specific parameters that we should modify. For instance, we set the maximum depth for <code class="docutils literal notranslate"><span class="pre">RandomForestWeightedNewsvendor</span></code> to 5 to avoid overfitting. Moreover, we set the number of neighbors
for <code class="docutils literal notranslate"><span class="pre">KNeighborsWeightedNewsvendor</span></code> to 30. After we have initialized our models we fit them on the training data and calculate the average cost on the test set by using the <code class="docutils literal notranslate"><span class="pre">score</span></code> function:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>cu = 15
co = 5

RFW = RandomForestWeightedNewsvendor(cu=cu, co=co, max_depth=5, criterion=&quot;mse&quot;,
                                     random_state=3)
RFW.fit(X_train, y_train[&quot;STEAK&quot;])
RFW_score = RFW.score(X_test,y_test[&quot;STEAK&quot;])
print(&quot;Avg. cost RFW: &quot;,RFW_score)

KNW = KNeighborsWeightedNewsvendor(cu=cu, co=co, n_neighbors=30)
KNW.fit(X_train_scaled, y_train[&quot;STEAK&quot;])
KNW_score = KNW.score(X_test_scaled,y_test[&quot;STEAK&quot;])
print(&quot;Avg. cost KNW: &quot;, KNW_score)

</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Avg. cost RFW:  [-47.76315789]
Avg. cost KNW:  [-47.47368421]
</pre></div></div>
</div>
<p>As we can see we were able to reduce the average costs again by using <strong>wSAA</strong>.</p>
</div>
<div class="section" id="Empirical-Risk-Minimization-(Version-2)">
<h3>Empirical Risk Minimization (Version 2)<a class="headerlink" href="#Empirical-Risk-Minimization-(Version-2)" title="Permalink to this headline">¶</a></h3>
<p>With wSAA we have already seen a data-driven approach that is able to consider features for decision making. However, to obtain a decision <span class="math notranslate nohighlight">\(q\)</span>, we have to first determine weights and then solve an optimization problem for every new sample <span class="math notranslate nohighlight">\(x\)</span>. But wouldn’t it be great to learn a function <span class="math notranslate nohighlight">\(q(x)\)</span> that instead maps directly from features <span class="math notranslate nohighlight">\(x\)</span> to decision <span class="math notranslate nohighlight">\(q\)</span>? We can do this by following the well-established machine learning principle of Empirical Risk Minimization
(ERM). The idea behind this approach is to find such a function by minimizing its empirical risk: :nbsphinx-math:<a href="#id15"><span class="problematic" id="id16">`</span></a>begin{equation}</p>
<blockquote>
<div><p>q(cdot)^*=min_{q(cdot)inmathcal{F}} R_n(q(cdot)),
tag{9}</p>
</div></blockquote>
<p>end{equation}`</p>
<p>where <span class="math notranslate nohighlight">\(R_n\)</span> is the empirical risk of function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> and <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is a function class. The empirical risk can be interpreted as the average distance between the prediction values and the true value on the training data. To calculate this difference a loss function <span class="math notranslate nohighlight">\(L(\cdot)\)</span> is used such that the ERM optimization problem can state as follow:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id17"><span class="problematic" id="id18">`</span></a>begin{equation}</dt><dd><p>min_{q(cdot)inmathcal{F}} R_n(q(cdot)) = min frac{1}{n}sum_{i=1}^{n}L(q(x_i),d_i)bigl],
tag{10}</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>where <span class="math notranslate nohighlight">\(x_i\)</span> is the feature vector of the ith training sample and <span class="math notranslate nohighlight">\(d_i\)</span> the corresponding true value. Recall that we want to obtain optimal inventory decisions, we can formulate the loss function according to the newsvendor problem. The optimization problem then becomes:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id19"><span class="problematic" id="id20">`</span></a>begin{equation}</dt><dd><p>min_{q(cdot)inmathcal{F}} R_n(q(cdot)) = min_{q(cdot)inmathcal{F}} frac{1}{n}sum_{i=1}^{n}bigl[cu(d_i-q(x_i))^+ + co(q(x_i)-d_i)^+bigl],
tag{11}</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>To solve this optimization problem we have to define the class to which function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> belongs. In the following part, we will see two approaches that define <span class="math notranslate nohighlight">\(q(\cdot)\)</span> in a different way.</p>
<div class="section" id="Classical-Empirical-Risk-Minimization-Newsvendor">
<h4>Classical Empirical Risk Minimization Newsvendor<a class="headerlink" href="#Classical-Empirical-Risk-Minimization-Newsvendor" title="Permalink to this headline">¶</a></h4>
<p>The first approach assumes that that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> belongs to the class of linear decision functions and is given by the weighted sum of features: <span class="math">\begin{equation}
q(\cdot)=\sum_{j=1}^{m}w_j*x_j=w^Tx,
\tag{12}
\end{equation}</span></p>
<div class="line-block">
<div class="line">where <span class="math notranslate nohighlight">\(m\)</span> is the total number of features and <span class="math notranslate nohighlight">\(w_j\)</span> is the weight of feature <span class="math notranslate nohighlight">\(j\)</span>. To find the function <span class="math notranslate nohighlight">\(q(\cdot)^*\)</span> that yileds optimal decisions we have to determine the corresponding feature weights. To do this we insert the decision function into eq. 11 and solve the following optimization problem:</div>
<div class="line"><span class="math">\begin{equation}
    \min \frac{1}{n}\sum_{i=1}^{n}\bigl[cu(d_i-w^T x_i)^+ + co(w^T x_i-d_i)^+\bigl],
    \tag{13}
\end{equation}</span></div>
</div>
<p>Once we have found the optimal weights, we can obtain a decision for a new sample simply by multiplying its feature values with the weights and then taking the sum.</p>
<p>We can apply this approach on the Yaz data by using the <code class="docutils literal notranslate"><span class="pre">EmpiricalRiskMinimization</span></code> class from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. First we initialize the class with our under- and overage cost. Then we fit the model on the training data, which means that we optimize the feature weights.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>ERM = EmpiricalRiskMinimizationNewsvendor(cu,co)
ERM.fit(X_train,y_train[&quot;STEAK&quot;])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
EmpiricalRiskMinimizationNewsvendor(co=5, cu=15)
</pre></div></div>
</div>
<p>Once the model is fitted we can extract the feature weights by accessing the <code class="docutils literal notranslate"><span class="pre">feature_weights_</span></code> attribute.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>weights = ERM.feature_weights_[0]
weights
</pre></div>
</div>
</div>
<p>Given the weights let us now determine a decision for the first sample of our test data. But before there is just one more thing we have take into account. We have 30 weights in total but only 29 features.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>print(&quot;Features: &quot;,X.shape[1],&quot; Weights: &quot;, weights.shape[0])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Features:  29  Weights:  30
</pre></div></div>
</div>
<p>This is due to the fact that our model needs a constant feature known as “intercept term”. Consequently, we add this term to the data sample.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>sample = pd.concat([pd.Series([1],index=[&quot;intercept&quot;]) , X_train.loc[0]])
sample
</pre></div>
</div>
</div>
<p>Finally, we can determine the decision as described bevore:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>sum(sample*weights)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
38.10295290249999
</pre></div></div>
</div>
<p>Instead of calculating the decision with the feature weights ourselves, we can simply use the model’s predict method.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>ERM.predict([X_train.loc[0]])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[38.1029529]])
</pre></div></div>
</div>
<p>To evaluate the modell we calculate the average cost on the whole test data by using the <code class="docutils literal notranslate"><span class="pre">score</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>ERM_score = ERM.score(X_test,y_test[&quot;STEAK&quot;])
print(&quot;Avg. cost ERM: &quot;, ERM_score)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Avg. cost ERM:  [-48.93235725]
</pre></div></div>
</div>
</div>
<div class="section" id="Deep-Learning-Newsvendor">
<h4>Deep Learning Newsvendor<a class="headerlink" href="#Deep-Learning-Newsvendor" title="Permalink to this headline">¶</a></h4>
<p>In the classical ERM approach we have assumed that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> belogs to the class of linear decision functions and is given by the weighted sum of our features as formulated in eq. 11. But maybe <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is more coplex, non-linear, and looks totally different. In comparison, the approach we will consider now uses a deep neural network to define the decision function. Consequently, <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is given by:</p>
<p><span class="math">\begin{equation}
q(\cdot)=\theta(x_i,w),
\tag{12}
\end{equation}</span> where <span class="math notranslate nohighlight">\(\theta(\cdot)\)</span> is the neural network function, <span class="math notranslate nohighlight">\(x_i\)</span> is the feature vector of the ith training sample and <span class="math notranslate nohighlight">\(w\)</span> is the matric of the network weights. How the function <span class="math notranslate nohighlight">\(\theta(\cdot)\)</span> looks like depends an the exact network architecture. However, inserting the decision function into the ERM optimization problem (eq. 11) will give us:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id21"><span class="problematic" id="id22">`</span></a>begin{equation}</dt><dd><p>min frac{1}{n}sum_{i=1}^{n}bigl[cu(d_i-theta(x_i,w))^+ + co(theta(x_i,w)-d_i)^+bigl],
tag{13}</p>
</dd>
</dl>
<p>end{equation}`</p>
<p>The goal of the network is then to find the weights <span class="math notranslate nohighlight">\(w\)</span> that minimize the loss function. Once the network has determined the optimal set of weights we can pass the feature vector of a new sample to the network, which inturn will output a decision.</p>
<p>To apply this approach we can use the <code class="docutils literal notranslate"><span class="pre">DeepLearningNewsvendor</span></code> class from <code class="docutils literal notranslate"><span class="pre">ddop</span></code>. We pass the under- and overage cost, fit the model on the training data and calculate the average cost on the test data by using the <code class="docutils literal notranslate"><span class="pre">score</span></code> function.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-none notranslate"><div class="highlight"><pre>
<span></span>DLN = DeepLearningNewsvendor(cu,co,verbose=0)
DLN.fit(X_train_scaled,y_train[&quot;STEAK&quot;])
DLN.score(X_test_scaled,y_test[&quot;STEAK&quot;])
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([-51.94359717])
</pre></div></div>
</div>
</div>
</div>
<div class="section" id="id23">
<h3>Summary<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h3>
<p>In this part of the tutorial we have seen three differnet data driven approaches to prescribe decisions directly from data. Let us now summarize what we have learned: - In the simplest case where we only have past demand observations we can use sample average approximation (SAA) to solve the newsvendor problem. The goal of SAA is to find the decision q that minimizes the average costs on past demand samples. - However, we have seen that additional demand features can improve decision making
since they usually reduce the degree of uncertainty. - With weighted sample average approximation (wSAA) and empirical risk minimization (ERM) we have then seen two data driven approaches that can take such features into account by using a function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> that maps from features <span class="math notranslate nohighlight">\(x\)</span> to a decision <span class="math notranslate nohighlight">\(q\)</span>. - WSAA defines the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> point-wise. It is based on deriving sample weights from features and optimizing SAA against a re-weighting of the training data.
To determine the weights we can use different weight functions e.g. based on k-nearest-neighbor regression,regression trees or random forest regression. - The ERM approach, on the other hand, tries to find the function <span class="math notranslate nohighlight">\(q(\cdot)\)</span> that maps directly from features to a decision by minimizing its empirical risk. Therefore, we have to specify the function class to which the decision function belongs. With the classical ERM newsvendor, and the deep learning newsvendor we have seen two methods
that define <span class="math notranslate nohighlight">\(q(\cdot)\)</span> in a different manner. While the former one assumes that <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is a linear decision function given by the weighted sum of features, the later one defines <span class="math notranslate nohighlight">\(q(\cdot)\)</span> as a deep neural network</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Andreas Philippi

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>